[{"title":"DDIA cookbook - (1)Reliable, Scalable, and Maintainable Application","url":"/2023/08/10/DDIA-cookbook-1-Reliable-Scalable-and-Maintainable-Application/","content":"IntroCPU is no longer the bottle neck. New problems are amount of data, complexity of data and the speed of changing. We call it data-intensive.\n\nStorage - Database\nSpeed - Cache\nSearch - Indexing\nUnkonwn size, continuous, asynchronous - Stream Processing\nAccumulated data - Batch Processing\n\nRight now, single tool is hard to meet requirements. And new tools are designed to optimize for variety of use cases, the boundary between category is blurred.\n\nReliabilityWhat is correct? It’s hard to define, but we can simply consider:\n\nThe App performs what user expected\nThe App can tolerate some mistakes or using it in unexpected ways\nThe App is good enough under certain load and data volume\nThe App prevents any abuse or unauthorized access\n\nThen we can define reliable means “continuing to work correctly, even bad things happend”.\nThe bad things are called faults, a reliable system should be fault-tolerant.\n\nfault vs failure\n\nfault - system deviates from original design\nfailure - system cannot work (crush)\n\nWe should design fault-tolerance mechanisms to prevent faults from causing failure\n\nHardware FaultsMorden hardware system will use RAID to add redundancy to reduce the fault rate.\nHardware faults are random and independent for most of the cases.\nSoftware ErrorsSoftware errors are sometimes correlated. And these bugs may hide for a long time until we trigger it.\n\nScalabilityScalability is used to describe a system’s ability to cope with increased load or changed resources.\nLoadRemember use case is always the key. Load can be:\n\nrequest per second\nread &#x2F; write ratio\nhit rate on cache\n…\n\nWhen we consider the load, the first thing is make the use case clear. There isn’t best solution, there is only suitable solution.\nPerformanceThere have two situations:\n\nIf load increases, and we keep the resource unchanged, how the performance changes?\nIf load increases, how many resources we need to change to keep the performance unchanged?\n\nTo solve these, we need to measure performance.\nThere are two key term: \n\nthroughput &rarr; the number of tasks we can process per second\nresponse time &rarr; the time between client sending request and receiving response\n\n\nlatency vs response time\n\nlatency &rarr; duration for a request waitting to be handled\nresponse time &rarr; user aspect, I send a request, how long it takes until I get response, it may include network delay, queuing delay, processing time, etc\n\n\nPercentileIf we run a request multiple times, the response time is not a fixed number, it has distribution. So average response time is p50 (50% percent).\nFor most of the response time, it looks good, so we always pay attention to tail latencies (high percentile), like p99 (90%) or p999 (99.9%). These response time is always very large and affect user’s experience.\nSLO SLAService Level Objectives &amp; Service Level Agreements are contracts that define the expected performance and avaliability of a service.\nFor example, some SLAs may define “p50 &lt; 50ms, p99 &lt; 100ms”.\nQueuing Delay &amp; HoLQueuing delay is one of the most significant reason for tail latency, because limited resource can only handle limited things in parallel.\nIf we have many requests, they will form a queue. Even the following 99% requests are fast, if the first 1% requests are slow, it will block the queue, and make the total execution time increasing. It’s called Head-of-Line block.\nApproaches for coping with loadScale up &rarr; vertical scale, means build more powerful machine\nScale out &rarr; horizontal scale, means distribute total load into several small machines\nElastic &rarr; autoscale, means this system can detect load changing, and automatically scale to keep performance\n\nMaintainability\nOperability - make it easy for operation team to keep the system running smoothly\nSimplicity - make new engineer can understand the system easily\nEvolvability - make it easy for adding new features\n\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (2)Data Models and Query Languages","url":"/2023/08/10/DDIA-cookbook-2-Data-Models-and-Query-Languages/","content":"Data ModelA data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities.\nMorden applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below it by providing a data model.\n\nRelational Vs Document ModelOne-to-Many\nFor relational model, is hard to represent One-to-Many relationship, like one person may have 0 to infinite work experience.\n\nRelational &rarr; create multiple table to store company info, school info, then use foreign key to JOIN several tables\nDocument &rarr; can use JSON-like structure, easy to read by human, and better locality (store these info in one place)\n\nMany-to-One &amp; Many-to-ManyWhen we store the region, we use ID rather than pure text. This is because ID has no meaning, it never needs to change. For example, if we want to update “Greater Seattle Area” to “Seattle”, we just need to modify the text in region_table.\nDocument model is good at One-to-Many because you can imagine it as a tree, but it’s not good at Many-to-X, because it looks like a graph.\nIf document model doesn’t support JOIN, then we need to use iteration to mock JOIN in application level. Even if it supports JOIN, we still need to use document reference (just like foreign key), which is similar to relational model.\nlocalityFor document database, it always store the whole document as a single object.\nFor read, it need to load the whole document from disk to memory, so if we need most of parts inside the document, it’s fine; otherwise, its performance is poor.\nFor write, it also need to rewrite the whole document from memory to disk, and only modifications that don’t change the total encoded size of a document can easily be performed in place; otherwise, system need to assign new space for new document.\nschema-on-read vs schema-on-writeDocument Database is not schemaless. Actually, it has implicit constrain, like when we write service code to read something from DB, we assume we can get some fields, so schema-on-read is a more accurate term.\n\n\n    schema-on-read\n    check when we READ\n    poor efficiency, cuz we cannot do any optimizations when we write it\n\n\n    schema-on-write\n    check when we WRITE\n    good efficiency cuz we can check the type then do optimization\n\n\n\nSummary\n\n\n\ndocument\nrelation\n\n\n\nrelation map\ntree, one-to-many\ncan use foreign key to achieve many-to-X\n\n\nJOIN\n:(\n:)\n\n\nflexibility\nflexible, can add fields easily\nschema, hard to change\n\n\nlocality\nif operate the whole doc, performance is good; but if only operate partial doc, performance is not good\nscatter in tables\n\n\n\nQuery LanguageDeclarative vs Imperative Language\n\n\n\nDeclarative\nImperative\n\n\n\nConcept\ndeclare the logic rather than actual execution\ndefine the execution plan\n\n\nExample\nSQL, CSS\nC++, Python, …\n\n\nAbstraction\nhigh\nlow\n\n\nParallel\ngood, cuz we let system do the optimization\npoor, cuz we already defined the steps\n\n\n\nWhat u want?\nHow to do that?\n\n\nHere are some advantages for declarative language:\n\nMore concise and easily use\nHide implementation details\nGood support for parallelism\n\nMapReduce QueryMapReduce is neither a declarative language nor a imperative language.\n\n\ndeclarative &rarr; we don’t need to specify how to iter or shuffle dataset\nimperative &rarr; we need to implement map and reduce functions\n\n\nIt requires the map and reduce are pure function, which means they only use input data, they cannot do anything else like query database.\nAnd they cannot have any side effects, which means no matter when we run the function for a given input, the output should be the same.\nWhat’s more, mapreduce is a very low-level model for distributed execution, so engineers can implement higher-level query language base it, like SQL can be implemented as a pipeline of mapreduce.\n\nGraph Data ModelSuitable for Many-to-Many relationships.\n\nvertice &#x2F; node\nedge &#x2F; relation\nattribute\n\nGraph can store both homogeneous and heterogeneous data. For example, node can represents people, city, animal, activity, etc.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (3)B-Tree and LSM-Tree","url":"/2023/08/17/DDIA-cookbook-3-B-Tree-and-LSM-Tree/","content":"LogHere, we define log as an append-only sequence of records.\n\nIndexIndex is an additional data structure, which doesn’t affect the contents of database, but affect the operation performance.\nIndex can speed up the read, but slow down the write cuz we need to maintain the index.\nHash IndexKey-Value StoreHash index is commonly used in Key-Value Storage. We can keep an in-memory hash map where every key is mapped to a offset in the data file. And we also store the k-v in log file which locates in disk. Every time we write a new k-v pair, we append the k-v in log file (notice log file in disk), and modify the index in memory. \nHowever, it may run out of disk space. A good way to solve this problem is Compaction.\nCompaction\nWe can store the index information in the segment, when the segment reach its size limitation, we make subsequent write in new segment. Then we can perform compaction process for these frozen segments. If same key appears many times in these segment, we just keep the newest value. It will generate the new compacted segments in new files and delete old segments to save space.\nThe whole process can happen in background thread, so it will not affect the service.\nTo find a value for a key, we just check the most recent segment, if the key is not present, then second-most-recent and so on.\nConsiderations\nDeleting Records &rarr; we need to keep the delete operation in logs (sometimes use special mark called tombstone)\nto mark the pervious operations for this key are useless, so if there is no new record for this key, this key shouldn’t appear in compacted segment.\nCrash Recovery &rarr; when database is restarted, in-memory hashmap is erased. We can iter all segment k-v info to rebuild the hashmap, but it’s costy. We can choose to store the k-v info and snapshots of the in-memory hashmap on disk, so that we just load the snapshots to rebuild hashmap.\nConcurrency Control &rarr; as writes are appended to the log in a strictly sequential order, a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by multiple threads.\n\nlog-structured vs update-in-placelog-structured &gt; update-in-place\n\nappend (sequential access) is always more efficient than random access\nconcurrency control and crash recovery are simpler &rarr; thanks for immutable log (or say it’s append-only)\ncompaction can reduce fragment problem\n\nlog-structured &lt; update-in-place\n\nkey must in memory\nkey is good for point searching but is bad for range searching\n\nSSTable &amp; LSM-TreeIf we let the key are sorted in segment, we call it Sorted String Table or SSTable.\nWhy SSTable is useful?\n\nMerging&#x2F;Compacting segments is simple and efficient. It use merge sort algorithm to achieve high efficiency. If one key appears in multiply segments, just use the most up-to-date segment value.\n\n\n\nSparse index. We don’t need to keep all keys in memory, instead, just keep sparse index.\nFor example, create key for “A”, and key for “C”, then key “B” must between these two.\n\n\nCompression. Since read requests need to scan over several key-value pairs in the requested range, it’s possible to group those records into a block and compress it before writing it to disk.\n\nConstruct and maintain SSTable\nWhen write comes in, add it to a in-memory data structure (red-black tree, AVL tree, etc). This in-memory tree is called memtable.\nThis data structure can insert unordered data, and dump ordered data, for simplicity, just imagine binary search tree, we can insert data in any order, then read them via pre-order traversal to get ordered output.\n\n\nWhen the memtable reachs its threshold, dump it into disk as a new SSTable (the data structure ensures when dump, it must be ordered). After SSTable is being written in disk, writes can continue to a new memtable.\nWhen read comes in, it check memtable first, then the most up-to-date SSTable, and so on.\nFrom time to time, a background process is running for compacting.\n\nThe only problem is: if crash happens, the memtable will lose all fresh data. To deal with it, we can add a log file in disk for memtable, if crash happens, just recover it from log file; if memtable dumps to disk, then delete the log file cuz it’s useless.\nMaking LSM-TreeLog-Structured Merge Tree is based on SSTable and memtable priciple. It will compact SSTable according to level or size.\n\nOptimization\nBloom Filter to aviod not exist keys.\nsize-tiered and level-tiered compaction.\n\n\nBloom Filter &rarr; it can judege that an element must not exist or possible exist\nFor a given input x, apply multiple hash functions to map its output in different place (you can imagine we have a vector&lt;bool&gt;), e.g. hash1(x), hash2(x), hash3(x). Then when we have a new input y, if hash1(y), hash2(y) and hash3(y) all have value (true in vector), which means y is possible exist; if any of these output don’t have value (false in vector), which means y must miss.\n\nThe basic idea of LSM Tree is: keeping a cascade of ordered SSTables that are merged in the background.\nBecause the data is sorted and sparse index can narrow the possible range, range query is also efficient, cuz you can use like binary search to boost query speed.\nB-TreeDifferent with LSM Tree who uses variable size of segments, B Tree breaks the database down into fixed size block or page, which aligns with disk design. So for any operations, the B-Tree uses page as unit.\nFor read, it’s just like find a value in binary search tree, whereas here is N-ary search tree, and the N called branching factor.\nFor update, it needs to load the whole page, update, then write the whole page back.\nFor write and delete, it may need to split &#x2F; merge node to make sure the tree is balanced.\nMaking tree reliableFor LSM Tree, the modifications will not change data in-place, instead, it will append new data and write to new place when compaction happens. But for B Tree, it wants to modify data in-place, which means modification doesn’t change the location of page.\nWhen the tree structure is adjusted, many pages may be modified in cascade. For example, after a leaf node is split, two new leaf nodes and a parent node need to be written (updating the leaf pointer).\n\ncrash &rarr; WAL (write ahead log), which means write the operation in log before execute it\nconcurrency control &rarr; latch (lighweight lock)\n\nOptimization\nInstead of WAL, use Copy-On-Write mechanism &rarr; instead of write page back to original place, create a new page in new place and modify parent’s pointer\nInstead of keep entire key, we can share prefix &rarr; for example, if the key is YYYYMMDD, the parent keeps YYYY, then its children keep MM, then DD. When we query, we must start from root, which means from left to right for key\nAdd pointers between leaf nodes to boost range query\n\nLSM-Tree vs B-TreeWrite Amplification &rarr; one write in database resulting in multiple writes to the disk.\nFor SSD, if there is a “delete” for some data, system cannot delete it immediately, system will give it a marker to indicate it’s useless. Remember for disk, the operation unit is page (or you can imagine it’s an area which contains many data), so when we execute a write:\n\nmove all useful data out of the page, and store them in somewhere\nerase all data in this page\nmove useful data back and add new data\n\nIt’s obviously writing more data than what we actually want, and this is the amplification.\n| | B-Tree | LSM-Tree || — | — | — | — || R&#x2F;W | read faster | write faster || Write Amplification | 1. data + WAL  2. massive data may cover different pages | 1. data + WAL  2. compaction needs to write in a new file || Write Throughput | low &rarr; random access | high &rarr; lower amplification; sequential access; compression makes smaller SSTable || Storage performance | lots of fragments | compaction save space || Bandwidth | predictable | compaction will take some bandwidth, which may infer service || Storage Amplification | some pages have unused space due to alignment | the same key exist in multiple segments || Concurrency | can add latch in parent nodes | one key exist in many places &rarr; MVCC |\nOther IndexPrimary vs Secondary| Id | Name | Age || --------------- || 1  | Tom  | 18  || 2  | Bob  | 18  || ...             |\n\nPrimary Index &rarr; index key is unique (such as primary key in MySQL), such as idSecondary Index &rarr; index key may be mapped to several values, such as age\nCluster vs Non-Cluster| Id | Name | Age || --------------- || 1  | Tom  | 18  || 2  | Bob  | 18  || ...             |\n\nCluster &rarr; index value is data, such as index[id=1] -&gt; &#123;id: 1, name: &quot;Tom&quot;, age: 18&#125;Non-Cluster &rarr; index value is pointer of data, such as index[age=18] -&gt; [id=1, id=2]\n\nFor MySQL\n\nprimary index is always cluster index &rarr; the leaf node of B-Tree is &lt;primary_key, row&gt;\nsecondary index is always non-cluster index &rarr; the leaf node of B-Tree is &lt;secondary_index, primary_key&gt;\n\n\n\nIn-Memory DBThe basic property for memory is: its data will lose after we restart the system. (except Non-Volatile Memory, aka NVM)\nWe can categorize in-memory DB into two categories:\n\nCache only &rarr; no persistency\nPersistency &rarr; use WAL, snapshot, replica for recovery and reload, but execute all operations in memory\n\nWhy in-memory DB is so efficient? Many people may think the biggest reason is it doesn’t need to communicate with disk. However, the truth is in-memory DB doesn’t need to decode&#x2F;encode data structures to fit into disk.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (3)OLTP, OLAP and Columnar Store","url":"/2023/08/21/DDIA-cookbook-3-OLTP-OLAP-and-Columnar-Store/","content":"OLTP &amp; OLAPOLTP &rarr; Online Transaction Processing.\nOLAP &rarr; Online Analytics Processing.\nA transaction needn’t have ACID. Transation processing just means allowing client to make low-latency reads and writes, which is opposed to batch processing.\nCompare\n\n\nProperty\nOLTP\nOLAP\n\n\n\nread\nsmall number of records per query, fetched by key\naggregate large amount of records\n\n\nwrite\nrandom access, low-latency\nbulk ELT or streaming\n\n\nprimary used by\nuesr, client\ninternal analyst for decision making\n\n\nwhat data represents\nlatest data state\nhistory of events that happened over time\n\n\ndata size\nsmall or medium\nlarge\n\n\nData Warehouse\nWarehouse will Extract-Transform-Load (ELT) data from multiple TP database and keep a read-only version data, so that when run AP in data warehouse, there is no inference with TP’s tasks.\nStars &#x2F; Snowflakes ModelStars model &rarr; center is the main table, and connect with several second level tables via foreign key\nSnowflakes model &rarr; similar to star model, but it may have more levels like third or fourth level to represent more detailed information\n\nColumn-orientedStorage Layout\nFor data warehouse, its main table may have 100+ even 1000+ columns, but for certain query, we just need roughly 2 ~ 3 columns for calculation. If we use row-oriented storage layout, it needs to fetch one row with 1000+ fields, then get only 2 ~ 3 data, then iter to next row. It has bad performance.\nIf we use column-oriented storage layout, it just need to fetch the required columns from disk and do aggregations.\nCompressionAnother advantage of column-oriented storage is: it can be compressed.\n\nfor column, it represents the same concept, so its domain may be small, for example, if column represents country, then it can only have 200 or 300 possible values\nfor column, it always has the same data type, like integer column, string column or boolean column, etc\n\nDue to these two properties, we can compress column via compression algorithm, e.g. Bitmask for bool, Run Length Encoding, etc.\nMemory bandwidthBecause column can be compressed, and for query we just fetch required columns without any useless fields, we can utilize memory bandwidth efficiently.\nVectorized processingSingle-Instruction-Multi-Data (a.k.a SIMD) technology can achieve vectorized processing, especially for bitwise operator, such as AND, OR, etc.\nv1 = [&quot;tom&quot;, &quot;bob&quot;, &quot;jack&quot;]bitmask = 0b010v1 SIMD bitmask = [None, &quot;bob&quot;, None]\n\nSort in column storage\nMost cases, we focus on aggregation of certain column, so the order is unnecessary\nSome cases, order can help us compress the column and do aggregation\nFor example, if we sort “age” column, then we can store the column as “(18, 100), (20, 50)” which means 100 rows have age&#x3D;18, 50 rows have age&#x3D;20.\n\n\nIt wouldn’t make sense to sort each column independently, because we will lose the information about which fields are in the same row\nDifferent queries benefit from different sort orders, so it can store the same data sorted in several ways\nFor example, in a cluster we have 3 machines, all of them can provide service. Then we can store data sorted by age in machine 1, sorted by name in machine 2, sorted by sex in machine 3.\n\n\n\nWriting to Column StorageAn update-in-place approach, like B-trees use, is not possible with compressed columns.\nWe can use LSM-Tree like method: All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn’t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk.\nAggregation: Data Cube &amp; Materialized ViewData warehouse queries often involve aggregations, such as COUNT, SUM, AVG, MIN, or MAX in SQL. If the same aggregates are used by many different queries, it can be wasteful to crunch through the raw data every time. So we can cache some of the counts or sums that queries use most often in disk.\nThe concept to persistent store something from memory into disk called materialize.\nThe materialized view is an actual copy of the query results, written to disk. When the underlying data changes, a materialized view needs to be updated, because it is a denormalized copy of the data.\nA common special case of a materialized view is known as a data cube, which is a grid of aggregates grouped by different dimensions.\n\n\nColumn familiy &ne; Column storageColumn family is a new concept for some databases like HBase and Cassandra.\nIts schema may looks like\n\n    \n        Id\n        Name\n        Work\n        Personal\n    \n    \n        Work.Phone\n        Work.Address\n        Personal.Phone\n        Personal.Address\n    \n    \n        1\n        Tom\n        xxx-xxx-xxxx\n        xxxx\n        yyy-yyy-yyyy\n        yyyy\n    \n    \n        2\n        Bob\n        xxx-xxx-xxxx\n        xxxx\n        yyy-yyy-yyyy\n        yyyy\n    \n\n\nThe column family means the Work.Phone and Work.Address are under Work family, the storage still uses row-oriented.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (4)Encoding","url":"/2023/08/25/DDIA-cookbook-4-Encoding/","content":"CompatibilityMorden software development will use rolling upgrade or staged rollout, which means deploying the new version to a few nodes at a time, checking whether the new version is bug free or not, then gradually upgrade all nodes.\nAnd also we need to consider that our client may not install new version for some time.\nSo the new and old data formats, codes, policies will coexist in our system.\n\nBackward compatibility &rarr; newer data can read data that written by older code\nForward compatibility &rarr; older data can read data that written by newer code\n\n\nEncodingIn memory, we have various data structures like hash map, dictionary, vector, etc. But when we want to write data to a file or send it over network, we have to transform the complex data structure to simple sequence of bytes.\nThe process to transform data structure to bytes called encoding &#x2F; serialization; the process to translate bytes to data structure called decoding &#x2F; deserialization.\nThe problems are:\n\nhow can we encode &#x2F; decode data to save time and space\nhow can we make the format be compatible\n\nthrift and protobufThey are binary encoding libraries that are based on the same principle. Both Thrift and Protocol Buffers require a schema for any data that is encoded.\nstruct Person &#123;    1: required string userName,    2: optional i64 favoriteNumber,    3: optional list&lt;string&gt; interests &#125;\n\nmessage Person &#123;    required string user_name = 1;    optional int64 favorite_number = 2;    repeated string interests = 3;&#125;\n\n\nNote: this format called Interface Defination Language(IDL).\n\nField tags and schema evolutionEach field is identified by its tag number (the numbers 1, 2, 3 in the sample schemas) and annotated with a datatype (like int64, string, etc). You can change the name of a field in the schema, since the encoded data never refers to field names, but you cannot change a field’s tag, since that would make all existing encoded data invalid.\nYou can add new fields to the schema, provided that you give each field a new tag number and make it optional or has default value.\n\n\nforward: If old code (which doesn’t know about the new tag numbers you added) tries to read data written by new code, including a new field with a tag number it doesn’t recognize, it can simply ignore that field. The datatype annotation allows the parser to determine how many bytes it needs to skip\nbackward: The new code can always read old data, because the tag numbers still have the same meaning. The only detail is that if you add a new field, you cannot make it required. If you were to add a field and make it required, that check would fail if new code read data written by old code, because the old code will not have written the new field that you added.\n\n\nRemoving a field is just like adding a field, you can only remove a field that is optional and you can never use the same tag number again.\nThe merits of schemas\nThey can be much more compact than the various “binary JSON” variants, since they can omit field names from the encoded data.s\nThe schema is a valuable form of documentation, and because the schema is required for decoding, you can be sure that it is up to date.\nKeeping a database of schemas allows you to check forward and backward compatibility of schema changes, before anything is deployed.\nFor users of statically typed programming languages, the ability to generate code from the schema is useful, since it enables type checking at compile time.\n\n\nModes of DataflowDataflow through DatabasesFor database, it may be accessed by several processes, some requests are old, some are new, so compatibility is very important. Sometimes DBMS may alter the table schema to add or delete fields, which may cause problem:\n\ndifferent values written at different timesIn database, you may have some values that were written five milliseconds ago, and some values that were written five years ago. When you change your service code (e.g. change the encoding policy), the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it since then. This observation is sometimes summed up as data outlives code.\nRewriting (migrating) data into a new schema is certainly possible, but it’s an expensive thing to do on a large dataset.\nMost relational databases allow simple schema changes, such as adding a new column with a null default value, without rewriting existing data. When an old row is read, the database fills in nulls for any columns that are missing from the encoded data on disk.\narchive dataWhen you take a snapshot of your database from time to time, say for backup purposes or for loading into a data warehouse. In this case, the data dump will typically be encoded using the latest schema, even if the original encoding in the source database contained a mixture of schema versions from different eras.\n\nSince you’re copying the data anyway, you might as well encode the copy of the data consistently.\n\nDataflow through service callsMicroservices &rarr; make the application easier to change and maintain by making services independently deployable and evolvable.\n\nFor example, each service should be owned by one team, and that team should be able to release new versions of the service frequently, without having to coordinate with other teams. In other words, we should expect old and new versions of servers and clients to be running at the same time.\n\nRESTfulREST (Representational state transfer) is not a protocol, but rather a design philosophy that builds upon the principles of HTTP.\nTL;DR &rarr; use URL to locate resources, use HTTP verbs to describe actions, use HTTP status codes to indicate results\n\nwww.myblog.com/introduce is a webpage related to introduce myself; www.myblog.com/blog/python/python_intro is a blog to introduce python\n\n\nGET means fetch data from server, POST means submit form from client, DELETE means delete resources in server, etc (although POST can also be achieved by GET, but we need to clarify our actions)\n\nReference &rarr; What is RESTful API\nUniform interfaceIt indicates that the server transfers information in a standard format. The formatted resource is called a representation in REST. This format can be different from the internal representation of the resource on the server application. For example, the server can store data as text but send it in an HTML representation format.\nUniform interface imposes four architectural constraints:\n\nRequests should identify resources. They do so by using a uniform resource identifier (URI).\nClients have enough information in the resource representation to modify or delete the resource if they want to. The server meets this condition by sending metadata that describes the resource further.\nClients receive information about how to process the representation further. The server achieves this by sending self-descriptive messages that contain metadata about how the client can best use them.\nClients receive information about all other related resources they need to complete a task. The server achieves this by sending hyperlinks in the representation so that clients can dynamically discover more resources.\n\nStatelessnessIn REST architecture, statelessness refers to a communication method in which the server completes every client request independently of all previous requests. Clients can request resources in any order, and every request is stateless or isolated from other requests. This REST API design constraint implies that the server can completely understand and fulfill the request every time. \nLayered systemIn a layered system architecture, the client can connect to other authorized intermediaries between the client and server, and it will still receive responses from the server. Servers can also pass on requests to other servers. You can design your RESTful web service to run on several servers with multiple layers such as security, application, and business logic, working together to fulfill client requests. These layers remain invisible to the client.\nCacheabilityRESTful web services support caching, which is the process of storing some responses on the client or on an intermediary to improve server response time. For example, suppose that you visit a website that has common header and footer images on every page. Every time you visit a new website page, the server must resend the same images. To avoid this, the client caches or stores these images after the first response and then uses the images directly from the cache. RESTful web services control caching by using API responses that define themselves as cacheable or noncacheable.\nCode on demandIn REST architectural style, servers can temporarily extend or customize client functionality by transferring software programming code to the client. For example, when you fill a registration form on any website, your browser immediately highlights any mistakes you make, such as incorrect phone numbers. It can do this because of the code sent by the server.\nRPCProblems\nA local function call is predictable and either succeeds or fails, depending only on parameters that are under your control. RPC is unpredictable: the request or response may be lost due to a network problem, or the remote machine may be slow or unavailable, and such problems are entirely outside of your control.\nA local function call either returns a result, or throws an exception, or never returns. RPC has another possible outcome: it may return without a result, due to a timeout.\nIf you retry a failed network request, it could happen that the requests are actually getting through, and only the responses are getting lost. In that case, retrying will cause the action to be performed multiple times.\nEvery time you call a local function, it normally takes about the same time to execute. A network request is much slower than a function call, and its latency is also wildly variable.\nWhen you call a local function, you can efficiently pass it references (pointers) to objects in local memory. When you make a network request, all those parameters need to be encoded into a sequence of bytes that can be sent over the network.\nThe client and the service may be implemented in different programming languages.\n\nEvolutionFor services dataflow, it is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, we only need backward compatibility on requests, and forward compatibility on responses.\nService compatibility is made harder by the fact that RPC is often used for communication across organizational boundaries, so the provider of a service often has no control over its clients and cannot force them to upgrade.\nFor RESTful APIs, common approaches are to use a version number in the URL or in the HTTP Accept header.\nDataflow through asynchronous message passing\nService &rarr; one process sends a request over the network to another process and expects a response as quickly as possible\nDatabase &rarr; one process writes encoded data, and another process reads it again sometime in the future\n\nThe asynchronous message-passing systems, which are somewhere between RPC and databases.\nThey are similar to RPC in that a client’s request (usually called a message) is delivered to another process with low latency. They are similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a message broker (a.k.a. message queue), which stores the message temporarily.\n\none database &rarr; store message in queue + two RPC &rarr; sender with queue, queue with receiver\n\nUsing a message broker has several advantages compared to direct RPC:\n\nIt can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.\nIt can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.\nIt avoids the sender needing to know the IP address and port number of the recipient.\nIt allows one message to be sent to several recipients.\nIt logically decouples the sender from the recipient because the sender just publishes messages and doesn’t care who consumes them.\n\nHowever, a difference compared to RPC is that message-passing communication is usually one-way: a sender normally doesn’t expect to receive a reply to its messages. This communication pattern is asynchronous: the sender doesn’t wait for the message to be delivered, but simply sends it and then forgets about it.\nmessage brokerMessage brokers are used as follows: one process sends a message to a named queue or topic, and the broker ensures that the message is delivered to one or more consumers of or subscribers to that queue or topic.\n\nseveral consumers share one topic (mutual exclusive)\nevery consumer own its topic\n\nA topic provides only one-way dataflow. However, a consumer may itself publish messages to another topic like a chain, or to a reply queue that is consumed by the sender of the original message. So by combining several topics together, we can achieve complex topology.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (5)Replication","url":"/2023/08/26/DDIA-cookbook-5-Replication/","content":"IntroWhy need multiple machines\nScalability &rarr; If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines.\nFault-tolerance&#x2F;high availability &rarr; If your application needs to continue working even if one machine goes down, you can use multiple machines to give you redundancy.\nLatency &rarr; If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them.\n\nScalingVertical ScalingVertical Scaling or Scaling up means upgrade to a more powerful machine.\nThere are two strategies:\n\nshared-memory &rarr; many CPUs use same memory and disk\nshared-disk &rarr; many CPUs and memories use same disk\n\nBut they have several problems:\n\nThe cost grows faster than linearly, add double hardware doesn’t mean double performance\nFor shared-memory, it’s limited to a single location\nFor shared-disk, the overhead of locking limit the scalability\n\nHorizontal scalingHorizontal Scaling or Scaling out treat every machine as node, each node uses its CPUs, RAM, and disks independently. Any coordination between nodes is done at the software level, using a conventional network.\nBecause every node is independent, so its strategy calls shared-nothing. It usually incurs additional complexity for applications and sometimes limits the expressiveness of the data models you can use.\nReplication Vs Partitioning\n\nReplication &rarr; Keeping a copy of the same data on several different nodes, potentially in different locations. Replication provides redundancy.\n\nPartitioning &rarr; Splitting a big database into smaller subsets called partitions so that different partitions can be assigned to different nodes (also known as sharding).\n\n\n\nLeader &amp; FollowerEach node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica; otherwise, the replicas would no longer contain the same data. The most common solution for this is called leader-follower&#x2F;master-slave replication.\n\n\nOne of the replicas is designated the leader (also known as master). When clients want to write to the database, they must send their requests to the leader.\nThe other replicas are known as followers (slaves or hot standbys). Whenever the leader get writes request, it also sends the data change to all of its followers as part of a replication log or change stream. Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.\nWhen a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader.\n\n\n\nSynchronous vs Asynchronous\n\nSync &rarr; leader waits until follower has confirmed that it received the write before reporting success to the user.\nAsync &rarr; leader sends the message, but doesn’t wait for a response from the follower.\n\nThe differences between sync and async are:\n\nsync sacrifices high availability to achieve strict consistency\nasync sacrifices strict consistency to achieve high availability\n\nHere are three configurations for different availability and consistency requirements:\n\nfully sync &rarr; leader finishes its write after all followers ack\nsemi sync &rarr; leader finishes its write after some of followers ack\nasync &rarr; leader finishes its write immediately, no need to wait follower ack\n\nSetting Up New FollowersFrom time to time, you need to set up new followers—perhaps to increase the number of replicas, or to replace failed nodes.\nWe cannot directly copy all current data files from other nodes because:\n\nclient will constantly write new data or update old data, if we only copy current data, it’s inconsistent\nif we block client’s write request, we can make all data consistent, but lose high availability\n\nThe correct apporach is:\n\nTake a consistent snapshot of the leader’s database at some point in time.\nCopy the snapshot to the new follower node.\nThe follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. The position of snapshot is sometimes called log sequence number or binlog coordinates.\nWhen the follower has processed the backlog of data changes since the snapshot, we say it has caught up. It can now continue to process data changes from the leader as they happen.\n\nHandling Node OutageFollower: catch-up recoveryEach follower keeps a log of the data changes it has received from the leader.\nThe follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected.\nLeader: failoverFailover\nDetermining that the leader has failed. The most common Failure detection algorithm uses timeout.\n\nChoosing a new leader. Leader election algorithm and Consensus algorithm: the leader is chosen by a majority of the remaining replicas and is usually the replica with the most up-to-date data changes from the old leader.\n\nReconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader.\n\n\nProblems\nIf asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If the former leader rejoins the cluster after a new leader has been chosen, the new leader may have received conflicting writes in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be discarded.\n\nDiscarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents.\n\nFor example current system needs to use Redis as cache, and the old leader wrote some primary key into Redis, then it crashed. The new leader also store its auto incremental primary key into Redis, but because its data is not up-to-date, so there may have some primary key already in Redis.\n\n\nIn certain fault scenarios, it could happen that two nodes both believe that they are the leader.\n\nHow to set a reasonable timeout value.\n\n\nImplementation of Replication LogsStatement-based replicationLeader records the SQL statements and send them to followers. The leader just acts like a client to there followers.\n\nFor example, the log may has recordSELECT * FROM table WHERE conditionsINSERT properties INTO table VALUES(values)\n\nThe disadvantages:\n\nNondeterministic &rarr; functions such as NOW() and RAND() are likely to generate a different value on each replica.\n\nExecution Order &rarr; if they use autoincrementing column, or if they depend on the existing data in the database, they must be executed in exactly the same order on each replica, or else they may have a different effect.\n\nSide effects &rarr; like triggers, stored procedures, user-defined functions may result in different side effects occurring on each replica, unless the side effects are absolutely deterministic.\n\n\nWrite-Ahead Log (WAL)WAL is used for recovery:\n\nIn the case of a log-structured storage engine, every modification is first written to a write-ahead log so that the memtable can be recovered even the crash happens.\n\nIn the case of a B-tree, which overwrites individual disk blocks, every modification is first written to a write-ahead log so that the index can be restored to a consistent state after a crash.\n\n\nIt’s append-only sequence of bytes. The disadvantage is:\n\nWAL contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine.\n\nIf the storage engine changes or is incompatible, WAL may cannot allocate the data to certain position.\nLogical Log (binlog in MySQL)A log should be decoupled from the storage engine.\nA logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row:\n\nFor an inserted row, the log contains the new values of all columns.\nFor a deleted row, the log contains enough information to uniquely identify the row that was deleted. (tombstone)\nFor an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns.\nFor transaction, the log will append a flag to inform the commit of the transaction.\n\nProblems with Replication LagIf we have multiple followers, we can get:\n\navailablity &rarr; tolerate some faults in other machine\nscalability &rarr; read can be distributed so we can deal with more requests\nlow-latency &rarr; request can choose a fast path\n\nBut the question is: how to make all replics look the same.\nIf we choose sync replication, all problems can be solved except that if some replics crash, the service is blocked to wait for replics recovery, which may cause user complain and is unacceptable.\nSo the only way is async replication. But it may has a problem: some replics may fall behind other replics.\nThis leads to apparent inconsistencies in the database: if you run the same query on the leader and a follower at the same time, you may get different results, because not all writes have been reflected in the follower.\nGood news is this inconsistency is just a temporary state—if you stop writing to the database and wait a while, the followers will eventually catch up and become consistent with the leader. For that reason, this effect is known as eventual consistency.\nReading Your Own Writes\nIn this situation, we need read-after-write consistency. This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time.\nMethods\nWhen reading something that the user may have modified, read it from the leader; otherwise, read it from a follower.\nTrack the time of the last update and, if new requests within certain time threshold, make all reads from the leader.\nThe client can remember the timestamp of its most recent write—then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp.\n\nProblems\nTimestamp is very hard to sync for one logic user has multiple physical device.\nReplicas are distributed across different datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter.s\n\nMonotonic Reads\nMonotonic reads is a guarantee that is a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency. When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.\n\nFor example, the old data is 1 &rarr; 2, and right now the newest data is 3, when we send the first read, it may return “2”, then I send several reads, it makes sure that the responses look like “2, 2, 2, …, 3, 3, 3”. “1” will never appear and if we see “3”, “2” will never appear.\n\nMethods\neach user always makes their reads from the same replica (different users can read from different replicas).\ntimestamp\n\nDifference with “Reading your own writes”“Reading your own writes” guarantees the read order after write, “Monotonic reads” guarantees multiple reads order.\nConsistent Prefix Reads (Causal)\nThe reason for inconsistent prefix is partition. The order inside one partition is easy to maintain, but inter partition is hard.\nMethods\nno partition.\nroute all causal requests to same partition, but how to detect several requests are causal is hard.\n\nSolutions for Replication LagTransaction!!! We will cover it later.\n\nMulti-leadersLeader-based replication has one major downside: there is only one leader, and all writes must go through it. So the extension for it is using multiple leaders.\n\nSingle leader vs Multiple leaders\n\n\n\nsingle leader\nmultiple leaders\n\n\n\nPerformance\nEvery write must go over the internet to the datacenter with the leader. This can add significant latency to writes.\nEvery write can be processed in the local datacenter and is replicated asynchronously to the other datacenters.\n\n\nTolerance of datacenter outages\nIf the datacenter with the leader fails, failover can promote a follower in another datacenter to be leader, the whole system needs to wait until new leader starts.\nEach datacenter can continue operating independently of the others, the failed datacenter can select new leader by their own.\n\n\nTolerance of network problems\nIt is very sensitive to public internet (inter-datacenter link).\nA multi-leader configuration with asynchronous replication can usually tolerate network problems better: a temporary network interruption does not prevent writes being processed.\n\n\nUse case\nClients with offline operation - Consider the calendar apps on your mobile phone, your laptop, and other devices. Every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas of your calendar on all of your devices.\nDatabase spanning multiple data centers.\nCollaborative editing - Like Google Doc.\n\nDownsideBut multi-leaders has a big problem: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved. So multi-leader is not a universal choice for all situations.\nHandling Write Conflicts\nConflict DetectionIn a single-leader database:\n\nthe second writer will either block and wait for the first write to complete\nabort the second write transaction, forcing the user to retry the write\n\nIn a multi-leader setup, both writes are successful, and the conflict is only detected asynchronously at some later point in time. At that time, it may be too late to ask the user to resolve the conflict.\nIn principle, you could make the conflict detection synchronous—i.e., wait for the write to be replicated to all replicas before telling the user that the write was successful. However, by doing so, you would lose the main advantage of multi-leader replication: allowing each replica to accept writes independently. If you want synchronous conflict detection, you might as well just use single-leader replication.\nConflict AvoidanceHandle conflicts &rarr; Avoid conflicts.\nThe core is: if the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur.\n\nFor example, in an application where a user can edit their own data, you can ensure that requests from a particular user are always routed to the same datacenter and use the leader in that datacenter for reading and writing. Different users may have different “home” datacenters, but from any one user’s point of view the configuration is essentially single-leader.\n\nThe only problem is: if the user changes the datacenter, e.g. move from new york to california or origin datacenter is down, the conflict occurs.\nConflict Converging (收敛)A single-leader database applies writes in a sequential order: if there are several updates to the same field, the last write determines the final value of the field. But for multi-leaders, every datacenter may has its own order, it hard to define a universal order.\nConvergent &rarr; means that all replicas must arrive at the same final value when all changes have been replicated.\n\nGive each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner. (if a timestamp is used, this technique is known as last write wins (LWW))\nGive each replica a unique ID, and let writes that originated at a highernumbered replica always take precedence over writes that originated at a lowernumbered replica.\nMerge the values together.\nRecord the conflict information and try to solve it later.\n\nCustom Conflict Resolution\nOn write\n\nAs soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler (callback function). This handler typically cannot prompt a user—it runs in a background process and it must execute quickly.\n\n\n\nOn read\n\nWhen a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict, and write the result back to the database.\n\n\nMulti-leaders Replication Topologies\n\n\n\n\nCircular\nStar &#x2F; Tree\nAll-to-All\n\n\n\nAdvantage\nTotal amount of replication messages is the same as amount of nodes, which is small.\n\nFault tolerance\n\n\nDisadvantage\nIf just one node fails, it can interrupt the flow of replication messages between other nodes, causing them to be unable to communicate until the node is fixed.\nThe same as circular.\n1. Total amount of replication messages is large  2. Because it doesn’t have order (no prior node or next node), the variation of latency of network links may cause causality problem.\n\n\n\nFor circular &amp; star, to aviod broadcast flooding, each node has a unique identifier, and in the replication log, each write needs to record all nodes it passed through.\n\n\nIf the latency of network links are different, the causality problem may occur.Because the update depends on the prior insert, so we need to make sure that all nodes process the insert first, and then the update. Simply attaching a timestamp to every write is not sufficient, because clocks sync is difficult. To order these events correctly, a technique called version vectors or vector clock, which is a logical clock. We will talk about that in next chapter.\n\n\nLeaderlessIn leaderless implementations:\n\nthe client directly sends its writes to several replicas.\na coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes.\n\nWriting to the Database When a Node Is DownIn a leaderless configuration, failover does not exist because it doesn’t have leader :).\nThe client sends the write to all replicas (let’s say n replicas) in parallel, and x available replicas accept the write but the n-x unavailable replicas miss it. We have a threshold h that if at least h replicas reply ok, we think the write is finished and we ignore the remaining n-h replicas. According to following figure, we can think x=2, n=3, h=2.\n\nWhen a client reads from the database, it doesn’t just send its request to one replica: read requests are also sent to several nodes in parallel. The client may get different responses from different nodes; i.e., the up-to-date value from one node and a stale value from another. Version numbers are used to determine which value is newer.\nRead repair and anti-entropy (AE)The replication scheme should ensure that eventually all the data is copied to every replica. After an unavailable node comes back online, how does it catch up on the writes that it missed?\n\nRead repair\nWhen a client makes a read from several nodes in parallel, it can detect any stale responses. When the client sees certain replica has a stale value, it writes the newer value back to that replica. This approach works well for values that are frequently read.\n\n\n\nAnti-entropy\nIn addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied.\n\n\nIn more basic terms, the AE service identifies missing or inconsistent shards and repairs them (it’s a DIFF process).\n\nAE can only perform its heroism when there is at least one copy of the shard still available. \nAE will not compare or repair hot shards, meaning that the shard can’t have active writes. Hot shards are more prone to change, and at any given moment, arrival of new data affects AE’s digest comparison.\n\n\n\n\nQuorums for reading and writingIf there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. As long as w + r &gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called quorum reads and writes.\nNormally, reads and writes are always sent to all n replicas in parallel. The parameters w and r determine how many nodes we wait for.\n\nWhat’s more, w and r are configurable, which means if the system is write heavy, we can set smaller w to reduce replication complexity.\nLimitations of Quorum ConsistencyEven w + r &gt; n, sometimes READ still gets stale values:\n\nIf a sloppy quorum is used, the n will change, which break the w + r &gt; n rule.\nIf two writes occur concurrently, it is not clear which one happened first. If we choose use timestamp to pick the winner, the clock skew will make replicas inconsistent.\nin leader-based strategy, leader will decide which is winner then send it to all replicas, but leaderless means every replica can has its own thought.\n\n\nIf a write happens concurrently with a read, the write may be reflected on only some of the replicas.\nIf a write succeeded on some replicas but failed on others and overall succeeded on fewer than w replicas, it is not rolled back on the replicas where it succeeded.\nIf a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may fall below w, breaking the quorum condition.\n\nMonitoring stalenessEven if your application can tolerate stale reads, you need to be aware of the health of your replication. If it falls behind significantly, it should alert you so that you can investigate the cause.\nFor leader-based replication, the database typically exposes metrics for the replication lag, which you can feed into a monitoring system. This is possible because writes are applied to the leader and to followers in the same order, and each node has a position in the replication log (the number of writes it has applied locally). By subtracting a follower’s current position from the leader’s current position, you can measure the amount of replication lag.\nFor leaderless replication, there is no fixed order in which writes are applied, which makes monitoring more difficult. Moreover, if the database only uses read repair (no anti-entropy), there is no limit to how old a value might be — if a value is only infrequently read, the value returned by a stale replica may be ancient.\n\nFor leaderless, we can write only partial replicas, so the write log in one replica cannot represent the actual write order for the whole system.\n\nSloppy Quorums and Hinted HandoffA network interruption can easily cut off a client from a large number of database nodes. Although those nodes are alive, and other clients may be able to connect to them, to a client that is cut off from the database nodes, they might as well be dead. In this situation, it’s likely that fewer than w or r reachable nodes remain, so the client can no longer reach a quorum.\nThere are two choices:\n\nlet all write and read fail\naccept write for now, but write it to some nodes that are reachable but aren’t among the original n quorum\n\nThe second choice is called sloppy quorum.\n\nFor example originally we choice 10 nodes from [1, …, 10], and if some machines are not reachable ([1, 2, 3, 4]), we write to [5, …, 10, 11*, 12*, 13*, 14*].\n\nSloppy quorums are particularly useful for increasing write availability: as long as any w nodes are available, the database can accept writes. However, this means that even when w + r &gt; n, you cannot be sure to read the latest value for a key, because the latest value may have been temporarily written to some nodes outside of n\nOnce the network interruption is fixed, any writes that one node temporarily accepted on behalf of another node are sent to the appropriate “home” nodes. This is called hinted handoff.\n\nFor example, when 1, 2, 3, 4 become available, the 11*, 12*, 13*, 14* will quit the quorum and the system still use [1, …, 10] as quorum. But the problem is the [1, …, 4] don’t have up-to-date values. In some systems, 11*-14* may send temporary stored updates back to 1-4 to make the original quorum works like nothing happend.\n\nMulti-datacenter operationThe number of replicas n includes nodes in all datacenters, and in the configuration you can specify how many of the n replicas you want to have in each datacenter. Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgment from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on the cross-datacenter link.\nOr keeps all communication between clients and database nodes local to one datacenter, so n describes the number of replicas within one datacenter. Cross-datacenter replication between database clusters happens asynchronously in the background, in a style that is similar to multi-leader replication.\nDetecting Concurrent WritesThe problem is that events may arrive in a different order at different nodes, due to variable network delays and partial failures. In order to become eventually consistent, the replicas should converge toward the same value.\nLast Write Wins (LWW)Even though the writes don’t have a natural ordering, we can force an arbitrary order on them. For example, we can attach a timestamp to each write, pick the biggest timestamp as the most “recent,” and discard any writes with an earlier timestamp. This conflict resolution algorithm, called last write wins (LWW).\nLWW achieves the goal of eventual convergence, but at the cost of durability: if there are several concurrent writes to the same key, even if they were all reported as successful to the client, only one of the writes will survive and the others will be silently discarded. Moreover, LWW may even drop writes that are not concurrent.\nIf losing data is not acceptable, LWW is a poor choice for conflict resolution.\nCausality (因果关系) vs ConcurrencyAn operation A happens before (causality) another operation B if B knows about A, or depends on A, or builds upon A in some way. So two operations are concurrent if neither happens before the other.\nIf one operation happened before another, the later operation should overwrite the earlier operation, but if the operations are concurrent, we have a conflict that needs to be resolved.\nFor defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if they are both unaware of each other, regardless of the physical time at which they occurred, because in distributed systems, clock skew cannot be aviod.\nCapture the causality relationship\n\nThe algorithm is:\n\nThe server maintains a version number Vi for every key, increments the version number Vi &rarr; Vi+1 every time that key is written, and stores the new version number along with the value written.\nWhen a client reads a key, the server returns all values that have not been overwritten, as well as the latest version number.\nWhen a client writes a key, it must include the version number Vx from the prior read, and it must merge together all values that it received in the prior read.\nWhen the server receives a write with a particular version number Vx, it can overwrite all values with that version number or below V &leq; Vx (since it knows that they have been merged into the new value), but it must keep all values with a higher version number (because those values are concurrent with the incoming write).\nIf a client writes a key without any version number, add it into current dataset.\n\nMerging concurrently written valuesThis algorithm ensures that no data is silently dropped, but it unfortunately requires that the clients do some extra work: if several operations happen concurrently, clients have to clean up afterward by merging the concurrently written values. A simple approach is to just pick one of the values based on a version number or timestamp (last write wins), but that implies losing data.\nWhen delete data, we cannot directly delete it, we need to set a tombstone to mark it as unavailable.\nVersion vectorUse a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas. This information indicates which values to overwrite and which values to keep as siblings. The version vector allows the database to distinguish between overwrites and concurrent writes.\n\n\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (6)Partitioning","url":"/2023/09/09/DDIA-cookbook-6-Partitioning/","content":"IntroThe main reason for wanting to partition data is scalability. Different partitions can be placed on different nodes in a shared-nothing cluster. Thus, a large dataset can be distributed across many disks, and the query load can be distributed across many processors.\nFor queries that operate on a single partition, each node can independently execute the queries for its own partition, so query throughput can be scaled by adding more nodes.\n\nPartition &amp; ReplicationPartitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. This means that, even though each record belongs to exactly one partition, it may still be stored on several different nodes for fault tolerance.\n\nEach partition’s leader is assigned to one node, and its followers are assigned to other nodes. Each node may be the leader for some partitions and a follower for other partitions.\n\nPartitioning of Key-Value DataThe goal of partition is to evenly distribute dataset among machines. If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed. A partition with disproportionately high load is called a hot spot.\nPartitioning by Key RangeOne way of partitioning is to assign a continuous range of keys to each partition.\n\nFor example, ‘a’-‘c’ in machine1, ‘d’-‘f’ in machine2, and so on.\n\nThe ranges of keys are not necessarily evenly spaced, because your data may not be evenly distributed. In order to distribute the data evenly, the partition boundaries need to adapt to the data.\n\nFor example, if ‘a’ and ‘e’ have more data, we can let ‘a’ in machine1, ‘b’-‘d’ in machine2, ‘e’ in machine3, and so on.\n\n\nAdvantage\nThe advantage of partitioning by key range is we can keep keys in sorted order (just like in SSTable). This has the advantage that range scans are easy, and you can treat the key as a concatenated index in order to fetch several related records in one query (just like yyyy-mm-dd).\n\n\n\nDisadvantage\nHowever, the downside of key range partitioning is that certain access patterns can lead to hot spots (because the order). For example, it we store the timestamp, and we always care about up-to-date value, then the machine stores new values will be hot spot.\n\n\nPartitioning by HashBecause of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key. A good hash function takes skewed data and makes it uniformly distributed.\nOnce you have a suitable hash function for keys, you can assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition’s range will be stored in that partition.\nThe partition boundaries can be evenly spaced, or they can be chosen pseudorandomly (in which case the technique is sometimes known as consistent hashing).\n\nDownside\nUnfortunately however, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries.\n\n\n\nCassandra achieves a compromise between the two partitioning strategies. A table in Cassandra can be declared with a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra’s SSTables.\n\nSkewed Workloads and Relieving Hot SpotsIn the extreme case where all reads and writes are for the same key, you still end up with all requests being routed to the same partition. (for example, on a social media site, a celebrity user with millions of followers may cause a storm of activity when they do something)\nA simple way to solve this problem is: if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions.\nHowever, having split the writes across different keys, any reads now have to do additional work, as they have to read the data from all 100 keys and combine it. You also need some way of keeping track of which keys are being split.\n\nPartitioning and Secondary IndexesA secondary index usually doesn’t identify a record uniquely but rather is a way of searching for occurrences of a particular value. The problem with secondary indexes is that they don’t map neatly to partitions.\nDocument Index (local)\nIn this indexing approach, each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition. It doesn’t care what data is stored in other partitions.\n\nAdvantage &rarr; when write, just add the seconday index in local partition.\nDisadvantage &rarr; when read, because we only konw the local state, so need to iter all partitions to gather same seconday index information.\n\nTerm Index (global)\nRather than each partition having its own secondary index, we can construct a global index that covers data in all partitions. However, we can’t just store that index on one node, since it would likely become a bottleneck and defeat the purpose of partitioning. A global index must also be partitioned, but it can be partitioned differently from the primary key index.\n\nAdvantage &rarr; when read, just read one partition.\nDisadvantage &rarr; when write, a write to a single document may now affect multiple partitions of the index (every term in the document might be on a different partition, on a different node).\n\n\nRebalancing PartitionsThe process of moving load from one node in the cluster to another is called rebalancing.\nThe requirements are:\n\nAfter rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster.\nWhile rebalancing is happening, the database should continue accepting reads and writes.\nNo more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I&#x2F;O load.\n\nStrategies for RebalancingThe simplest way to do partition is hash by mod n, which means key % N &#x3D; partition id. But the problem is: when we do rebalancing due to some nodes fail or change, the N will change, and most of the keys will need to be moved.\nFixed number of partitionsCreate many more partitions than there are nodes, and assign several partitions to each node.\n\nFor example, we have 10 nodes and 100 partitions, then every node stores 10 partitions.\n\nOnly entire partitions are moved between nodes. The number of partitions does not change, nor does the assignment of keys to partitions (we don’t change content of the partition, we just move it as a whole group). The only thing that changes is the assignment of partitions to nodes.\n\nThis change of assignment is not immediate, so the old assignment of partitions is used for any reads and writes that happen while the transfer is in progress.\nIn this configuration, the number of partitions is fixed when the database is first set up and not changed afterward. Although in principle it’s possible to split and merge partitions (see the next section), a fixed number of partitions is operationally simpler. Thus, the number of partitions configured at the outset is the maximum number of nodes you can have (make sure every nodes have at least more than one partition).\n\nDisadvantage &rarr; You need to choose the fixed number high enough to accommodate future growth (because the number cannot be changed). However, each partition also has management overhead, so it’s counterproductive to choose too high a number. It’s pretty hard to configure in advance especially when the data may change drasticly.\n\nDynamic partitioningWhen a partition grows to exceed a configured size, it is split into two partitions so that approximately half of the data ends up on each side of the split. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be merged with an adjacent partition.\nEach partition is assigned to one node, and each node can handle multiple partitions, like in the case of a fixed number of partitions. After a large partition has been split, one of its two halves can be transferred to another node in order to balance the load.\n\nAdvantage &rarr; The number of partitions adapts to the total data volume.\n\nDisadvantage &rarr; An empty database starts off with a single partition, since there is no a priori information about where to draw the partition boundaries. So all read and write will hit the same node while other nodes are idle.\n\n\nPartitioning proportionally to nodes (Consistent Hash)For fixed partition, the size of each partition is proportional to the size of the dataset. For dynamic partition, the the number of partitions is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.\nA third option is to make the number of partitions proportional to the number of nodes—in other words, to have a fixed number of partitions per node. In this case, the size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged, but when you increase the number of nodes, the partitions become smaller again. Since a larger data volume generally requires a larger number of nodes to store, this approach also keeps the size of each partition fairly stable.\nWhen a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. The randomization can produce unfair splits, but when averaged over a larger number of partitions, the new node ends up taking a fair share of the load from the existing nodes.\n\nPicking partition boundaries randomly requires that hash-based partitioning is used.\n\nWhen an old node leave the cluster, it also rebalances its partitions to other nodes, and do partition merging process.\nOperations: Automatic or Manual RebalancingFully automated rebalancing can be convenient, because there is less operational work to do for normal maintenance. However, it can be unpredictable. Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.\nFor that reason, it can be a good thing to have a human in the loop for rebalancing. It’s slower than a fully automatic process, but it can help prevent operational surprises.\n\nRequest RoutingWhen a client wants to make a request, how does it know which node to connect to? As partitions are rebalanced, the assignment of partitions to nodes changes.\nThis is an instance of a more general problem called service discovery. There are several ways:\n\nAllow clients to contact any node. If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.\n\nSend all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.\n\nRequire that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.\n\n\nIn all cases, the key problem is: how does the component making the routing decision (which may be one of the nodes, or the routing tier, or the client) learn about changes in the assignment of partitions to nodes?\nMany distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.\n\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (7)Transactions","url":"/2023/09/17/DDIA-cookbook-7-Transactions/","content":"Concept of TransactionA transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback).\nNot every application needs transactions, and sometimes there are advantages to weakening transactional guarantees or abandoning them entirely (for example, to achieve higher performance or higher availability).\nACIDAtomicityFor a group of operations, if no fault occurs, then execute all operations and change the state (commit); otherwise, no operation will be executed, keep original state (abort).\n\nDifferent with atomic in multi-threaded.\nIn multi-threaded programming, if one thread executes an atomic operation, that means there is no way that another thread could see the half-finished result of the operation. The system can only be in the state it was before the operation or after the operation, not something in between.\nFor ACID, it does not describe what happens if several processes try to access the same data at the same time.\n\nConsistencyThe idea of ACID consistency is that you have certain statements about your data (invariants) that must always be true, if a transaction starts with a database that is valid according to these invariants, and any writes during the transaction preserve the validity, then you can be sure that the invariants are always satisfied.\n\nThis idea of consistency depends on the application’s notion of invariants, and it’s the application’s responsibility to define its transactions correctly so that they preserve consistency. This is not something that the database can guarantee.\n\n\nThis consistency is not the same as consistency in distributed systems (called CAP). ACID consistency is a user-defined rule, while CAP consistency is “request to any nodes will get the same response”.\n\nIsolationAny read or write in the same transaction will not be affected by other transactions.\nFor details, please refer to “Concurrent Operations &amp; Isolation Levels” section.\nPossible Problems\n\nDirty Read &rarr; A Dirty read is a situation when a transaction reads data that has not yet been committed.\nNon Repeatable read &rarr; Non Repeatable read occurs when a transaction reads the same row twice and gets a different value each time.\nPhantom Read &rarr; Phantom Read occurs when two same queries are executed, but the rows retrieved by the two, are different.\n\n\nDifference between non repeatable and phantom is: non repeatable focus on certain row, and the returned value of that row; phantom focus on same query and the returned set.\n\nIsolation levelsLevels from loose to strict:\n\nRead Uncommitted (no isolation) &rarr; In this level, one transaction may read not yet committed changes made by other transactions, thereby allowing dirty reads.\nRead Committed &rarr; This isolation level guarantees that any data read is committed at the moment it is read. Thus it does not allow dirty read. The transaction holds a read or write lock on the current row, and thus prevents other transactions from reading, updating, or deleting it.\nRepeatable Read &rarr; The transaction holds read locks on all rows it references and writes locks on referenced rows for update and delete actions. Since other transactions cannot read, update or delete these rows, consequently it avoids non-repeatable read.\nSerializable (purely like ordered transactions) &rarr; A serializable execution is guaranteed to be serializable. Serializable execution is defined to be an execution of operations in which concurrently executing transactions appears to be serially executing.\n\nDurabilityDurability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.\n\nsingle-node &rarr; write data into disk or WAL for recovery.\nmulti-nodes &rarr; data has been successfully copied to some number of nodes.\n\nBASESystems that do not meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency.\nSingle-Object &amp; Multi-Object Operationssingle-object writesSingle object modification means multiple threads may access the same value (row, table, etc). Atomicity can be implemented using a log for crash recovery and isolation can be implemented using a lock on each object.\n\nAtomicity for single object may be hard to understand because only one operation happens, you can imagine it like “write a very large data chunk into disk”, there are two outcomes: (1) all data in disk; (2) no data in disk.\n\nSome databases also provide more complex atomic operations, such as an increment operation, which removes the need for a read-modify-write cycle. Similarly popular is a compare-and-set operation, which allows a write to happen only if the value has not been concurrently changed by someone else. We will cover this later.\nThese single-object operations are useful, as they can prevent lost updates when several clients try to write to the same object concurrently. However, they are not transactions in the usual sense of the word, because transaction means we operate more than one objects.\nmulti-object transactionsSome distributed databases abandon multi-object transactions because it’s difficult to implement across partitions and may influent performance and availability.\nBut there are some situations that still require multi-object transactions, for example:\n\nforeign key constrain\nsecondary index\n\nHandling errors and abortsACID databases are based on this philosophy: if the database is in danger of violating its guarantee of atomicity, isolation, or durability, it would rather abandon the transaction entirely than allow it to remain half-finished.\nNot all systems follow that philosophy, though. In particular, datastores with leaderless replication work much more on a “best effort” basis, which could be summarized as “the database will do as much as it can, and if it runs into an error, it won’t undo something it has already done”—so it’s the application’s responsibility to recover from errors.\nRetrying an aborted transaction is a simple and effective error handling mechanism, it isn’t perfect:\n\nIf the transaction success but the network fails to response to client, then retry may execute same transaction twice.\nIf the error is due to overload, then retry will make the problem worse.\nIt’s only worth retrying after transient errors, like deadlock, temporary network break, etc. For permanent error such as constraint violation, retrying is meaningless.\nIf the transaction has side effect, then even transaction failed, the side effect may affect other part already.\nIf the client fails while retrying, then everything is lost.\n\n\nWeak Isolation LevelsRead CommittedDirty ReadNo dirty read &rarr; When reading from the database, you will only see data that has been committed.\nWhy need to avoid dirty read?\n\nIf a transaction needs to update several objects, a dirty read means that another transaction may see some of the updates but not others.\nFor example, the user sees the new unread email but not the updated counter. This is a dirty read of the email. Seeing the database in a partially updated state is confusing to users and may cause other transactions to take incorrect decisions.\n\n\nIf a transaction aborts, any writes it has made need to be rolled back. If the database allows dirty reads, that means a transaction may see data that is later rolled back.\n\nDirty WriteNo dirty write &rarr; When writing to the database, you will only overwrite data that has been committed.\nWe normally assume that the later write overwrites the earlier write. However, what happens if the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value? This is called a dirty write.\nImplement Read CommittedMost commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. It must then hold that lock until the transaction is committed or aborted. Only one transaction can hold the lock for any given object; if another transaction wants to write to the same object, it must wait until the first transaction is committed or aborted before it can acquire the lock and continue.\nWhat about preventing dirty reads?\n\nOne option is use the same read lock, but it’s not a good idea, because one long-running write transaction can force many read-only transactions to wait until the long-running transaction has completed. This harms the response time of read-only transactions and is bad for operability.\nAnother option is keeping both committed and uncommitted values.  For every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.\n\nSnapshot Isolation and Repeatable Read\nThis is called non repeatable read.\nSnapshot isolation is the most common solution to this problem. The idea is that each transaction reads from a consistent snapshot of the database—that is, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.\nImplement Snapshot Isolation (MVCC)Like read committed isolation, implementations of snapshot isolation typically use write locks to prevent dirty writes.\nHowever, reads do not require any locks. From a performance point of view, a key principle of snapshot isolation is readers never block writers, and writers never block readers. This allows a database to handle long-running read queries on a consistent snapshot at the same time as processing writes normally, without any lock contention between the two.\nThe database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as multiversion concurrency control (MVCC).\n\nEach row in a table has a created_by field, containing the ID of the transaction that inserted this row into the table.\nMoreover, each row has a deleted_by field, which is initially empty. If a transaction deletes a row, the row isn’t actually deleted from the database, but it is marked for deletion by setting the deleted_by field to the ID of the transaction that requested the deletion. At some later time, when it is certain that no transaction can any longer access the deleted data, a garbage collection process in the database removes any rows marked for deletion and frees their space.\nAn update is internally translated into a delete and a create.\nVisibility rulesWhen a transaction reads from the database, transaction IDs are used to decide which objects it can see and which are invisible.\n\nAt the start of each transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.\nAny writes made by aborted transactions are ignored.\nAny writes made by transactions with a later transaction ID (i.e., which started after the current transaction started) are ignored, regardless of whether those transactions have committed.\nAll other writes are visible to the application’s queries.\n\nIn other words, an object is visible if both of the following conditions are true:\n\nAt the time when the reader’s transaction started, the transaction that created the object had already committed.\nThe object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader’s transaction started.\n\nIndexes\nThe index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction. When garbage collection removes old object versions that are no longer visible to any transaction, the corresponding index entries can also be removed.\nUse an append-only&#x2F;copy-on-write variant that does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page. Parent pages, up to the root of the tree, are copied and updated to point to the new versions of their child pages.\n\nPreventing Lost UpdatesThere are several interesting kinds of conflicts that can occur between concurrently writing transactions. The best known of these is the lost update problem.\n\nThe lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.\nAtomic write operaitonsMany databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code.\n\nThis means a new operation always happens after the old operation finishing.\n\nAtomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. Another option is to simply force all atomic operations to be executed on a single thread.\nUnfortunately, object-relational mapping frameworks make it easy to accidentally write code that performs unsafe read-modify-write cycles instead of using atomic operations provided by the database.\n\nAlthough DB provide threading-safe SQL commands, user may use the DB in a wrong way and violate the atomicity :(.\n# threading-safeMyDB.execute(&quot;UPDATE counters SET value = value + 1 WHERE key = &#x27;foo&#x27;;&quot;)# violate atomicityvalue = MyDB.execute(&quot;SELECT value FROM counters WHERE key = &#x27;foo&#x27;;&quot;)new_value = value + 1MyDB.execute(&quot;UPDATE counters SET value = new_value WHERE key = &#x27;foo&#x27;;&quot;)\n\nExplicit lockingAdd lock in your own code.\nwith resource.get_lock() as re:    do_something()\n\nAutomatically detecting lost updatesAllow to run operations in parallel, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.\nCompare and SetCompare-and-set operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. If the current value does not match what you previously read, abort the update, and the read-modify-write cycle must be retried.\nConflict resolution and replicationLocks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, databases with multi-leader or leaderless replication usually allow several writes to happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a single up-to-date copy of the data. Thus, techniques based on locks or compare-and-set do not apply in this context.\nSolution &rarr; allow concurrent writes to create several conflicting versions of a value (also known as siblings), and to use application code or special data structures to resolve and merge these versions after the fact.\nWrite Skew and PhantomsWrite SkewIt is neither a dirty write nor a lost update, because the two transactions are updating two different objects.\nYou can think of write skew as a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly.\nPhantomsA write in one transaction changes the result of a search query in another transaction, is called a phantom. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions, phantoms can lead to particularly tricky cases of write skew.\nThe general steps that cause phantoms are:\n\nA SELECT query checks whether some requirement is satisfied by searching for rows that match some search condition.\nDepending on the result of the first query, the application code decides how to continue\nIf the application decides to go ahead, it makes a write (INSERT, UPDATE, or DELETE) to the database and commits the transaction\n\nMaterializing conflictsMaterializing conflicts &rarr; takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database.\nIt can be hard and error-prone to figure out how to materialize conflicts, and it’s ugly to let a concurrency control mechanism leak into the application data model.\n\nSerializabilitySerializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.\nMost databases that provide serializability today use one of three techniques:\n\nactual serial execution\ntwo-phase locking\nconcurrency control\n\nActual Serial ExecutionThe simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.\n\nIt seems very straight forward, why this appears only recently?\n\nRAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory.\nOLTP transactions are usually short and only make a small number of reads and writes. For long-running analytics quries, they are typically read-heavy and can use snapshot.\n\n\nA system designed for single-threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking.\nHowever, its throughput is limited to that of a single CPU core. In order to make the most of that single thread, transactions need to be structured differently from their traditional form.\nEncapsulating transactions in stored proceduresIf a database transaction needs to wait for input from a user, the database needs to support a potentially huge number of concurrent transactions, most of them idle. Most databases cannot do that efficiently, and so almost all OLTP applications keep transactions short by avoiding interactively waiting for a user within a transaction.\nIn this interactive style of transaction, a lot of time is spent in network communication between the application and the database. If you were to disallow concurrency in the database and only process one transaction at a time, the throughput would be dreadful because the database would spend most of its time waiting for the application to issue the next query for the current transaction.\n\nSystems with single-threaded serial transaction processing don’t allow interactive multi-statement transactions. Instead, the application must submit the entire transaction code to the database ahead of time, as a stored procedure.\n\nNormal process is running multiple queries one by one, which looks like execute multiple commands in console;Stored procedure is packing multiple queries together and send this batch via network, which looks like running script.\n\nPros and cons of stored proceduresSome cons:\n\nEach database vendor has its own language for stored procedures\nHard to debug, monitor, test, version control, etc\nA badly written stored procedures in the database may cause much more trouble than bad code in applications, because one database may be used by several applications\n\nSome pros:\n\nWith stored procedures and in-memory data, executing all transactions on a single thread becomes feasible. As they don’t need to wait for I&#x2F;O and they avoid the overhead of other concurrency control mechanisms, they can achieve quite good throughput on a single thread.\nSome database use stored procedures for replication: instead of copying a transaction’s writes from one node to another, they execute the same stored procedure on each replica. (no need to transfer data, just transfer the script or “how to get these data”)\n\nPartitioningExecuting all transactions serially makes concurrency control much simpler, but limits the transaction throughput of the database to the speed of a single CPU core on a single machine. Read-only transactions may execute elsewhere, using snapshot isolation, but for applications with high write throughput, the single-threaded transaction processor can become a serious bottleneck.\nIn order to scale to multiple CPU cores, and multiple nodes, you can potentially partition your data.\n\nIf you can find a way of partitioning your dataset so that each transaction only needs to read and write data within a single partition, then each partition can have its own transaction processing thread running independently from the others. In this case, you can give each CPU core its own partition, which allows your transaction throughput to scale linearly with the number of CPU cores.\nHowever, for any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions that it touches. The stored procedure needs to be performed in lock-step across all partitions to ensure serializability across the whole system.\n\nSummarySerial execution of transactions has become a viable way of achieving serializable isolation within certain constraints:\n\nEvery transaction must be small and fast, because we only have one tread, low-speed transaction will block following tasks.\nIt is limited to use cases where the active dataset can fit in memory so that no need to wait to load data from disk.\nWrite throughput must be low enough to be handled on a single CPU core.\nRare cross-partition transations.\n\nTwo-Phase Locking (2PL)NOTE: TWO-PHASE LOCKING IS DIFFERENT WITH TWO-PHASE COMMIT!!!\nSeveral transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write (modify or delete) an object, exclusive access is required:\n\nIf transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue.\nIf transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue.\n\nImplementationThere are 2 lock types: shared lock (read lock) and exclusive lock (write lock).\n\nReaders &rarr; acquire and hold shared lock, multiple readers can share the lock on the same object. If the object already has an exclusive lock, then readers need to wait.\nWriter &rarr; acquires and holds exclusive lock. One object can only has one exclusive lock, so if there is any existing lock on the object, the transaction must wait.\nLock Upgrade &rarr; If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock.\nAfter a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort).\n\n\nOne of the problem of 2PL is deadlock. The most common solution is detecting the deadlocks between transactions then aborting one of them to break the tie.\n\nPerformanceWhy 2PL is not a ultimate solution? The transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation.\n\nThe overhead of acquiring and releasing all those locks.\nReduce the concurrency. Some transactions need to wait other to finish first.\nDeadlock. If we choose to abort some transations, the redo processes are costy.\n\nLocksThere are several choices for locks, they have different granularities.\nRow-based locksThis lock will lock the certain rows. It has the finest granularity, but the performance is not good, because there may exist lots of locks.\nCondition-based locks (Predicate locks)It works similarly to the shared&#x2F;exclusive lock described earlier, but rather than belonging to a particular object (e.g., one row in a table), it belongs to all objects that match some search condition.\nThe key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable.\nThe problem of predicate locks is its performance: if there are many locks by active transactions, checking for matching locks becomes time-consuming.\nIndex-based locksFor example, if you have a predicate lock for bookings of room 123 between noon and 1 p.m., you can approximate it by locking bookings for room 123 at any time, or you can approximate it by locking all rooms (not just room 123) between noon and 1 p.m.\n\nImagine it has index for time and for room, so the index-range lock will lock the whole index (time or room).\n\nIt’s not very precise, but since it has much lower overheads, it’s a good compromise.\nConcurrency ControlThere are two types of Concurrency Control policy:\n\nPessimistic &rarr; if anything might possibly go wrong, it’s better to wait until the situation is safe again before doing anything.\nOptimistic &rarr; instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks whether anything bad happened.\n\nSerializable Snapshot Isolation (SSI) is one of the most famous optimistic algorithm. It based on snapshot, every transaction runs in its own snapshot, then before committing, checking the conflicts.\n\nWhy wait until committing? Why not abort transaction immediately when the conflict is detected?Because if other transaction aborted, then no conflict exist, or current transaction is read-only.\n\nWe call the snapshot in the start of transaction as premise. Because a typical transaction is read-modify-write, and we use snapshot to isolate changes in other transactions, so the premise maybe out-of-date when we commit this transaction (for example, other transaction modified some values and committed before this transaction).\nAny changes to the results of the read or query may invalidate the writes in the transaction. That is, the database must know that the writes in this transaction are based on an outdated premise, and then abort the transaction.\nHow does the database know whether the query results may have changed? There are two situations to consider.\n\nWhen the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted, because this transaction based on an outdated (changed) premise.\nIn this example, Tx 42 modified the “Alice” but not yet commit, so in the view of Tx 43, this modification is ignored, it still treat “Alice” &#x3D; true. Then Tx 43 makes its changes in “Bob”. After Tx 42 is committed, the premise for Tx 42 is outdated (“Alice” from true to false), so any writes in Tx 43 are invalid (because any writes in Tx 43 are based on its premise, the premise has changed, so the writes maybe invalid).\n\nIn this example, the premises for Tx 42 and Tx 43 are the same. After Tx 42 modified “Alice”, Tx 43 also wants to modify “Bob”. Because no Tx is committed, so Tx 42 and 43 are modifying different premises (snapshots), no conflict. When Tx 42 commits, the premise doesn’t change, so it is approved. When Tx 43 commits, the premise is outdated (“Alice” from true to false), so it is rejected.\nPerformanceCompared to 2PL, the big advantage of SSI is that one transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don’t block readers, and vice versa. This design principle makes query latency much more predictable and less variable. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.\nCompared to serial execution, SSI is not limited to the throughput of a single CPU core. Even though data may be partitioned across multiple machines, transactions can read and write data in multiple partitions while ensuring serializable isolation.\nThe rate of aborts significantly affects the overall performance of SSI. For example, a transaction that reads and writes data over a long period of time is likely to run into conflicts and abort, so SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay).\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (8)The Trouble with Distributed Systems","url":"/2023/11/01/DDIA-cookbook-8-The-Trouble-with-Distributed-Systems/","content":"Faults and Partial FailuresFor single computer system, it is usually either fully functional or entirely broken. System prefers to crash completely rather than return some “error message”.\nIn a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine. This is known as a partial failure. The key of distributed system is nondeterministic.\nSupercomputingSupercomputing (high-performance computing, HPC) &rarr; focus on intensive scientific computing tasks with the help of thousands of CPUs and powerful machines.\nIn a supercomputer, a job typically checkpoints the state of its computation to durable storage from time to time. If one node fails, a common solution is to simply stop the entire cluster workload. After the faulty node is repaired, the computation is restarted from the last checkpoint. Thus, a supercomputer is more like a single-node computer than a distributed system: it deals with partial failure by letting it escalate into total failure—if any part of the system fails, just let everything crash.\nCloud ComputingCloud computing &rarr; focus on multi-tenant datacenters, commodity computers connected with an IP network and elastic&#x2F;on-demand resource allocation.\nIf we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software.\n\nUnreliable NetworksIn distributed systems, we always use shared-nothing architecture, so the network is the only way those machines can communicate.\nThe internet and most internal networks in datacenters are asynchronous packet networks. In this kind of network, one node can send a message (a packet) to another node, but the network gives no guarantees as to when it will arrive, or whether it will arrive at all.\n\nYour request may have been lost;\nYour request may be waiting in queue;\nRemote node may have failed;\nRemote node may have processed your request, but the response has been lost;\nRemote node may have processed your request, but the response has been delayed due to traffic;\n\nThe usual way of handling this issue is a timeout: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don’t know whether the remote node got your request or not.\nTimeout and Unbounded DelaysThere are two metrics:\n\nAccuracy &rarr;  every detected failure corresponds to a crashed process (no mistakes)\nCompleteness &rarr; every process failure is eventually detected (no misses)\n\nA short timeout detects faults faster, it has high completeness, but carries a higher risk of incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown, which means it has low accuracy. Vice versa for a long timeout.\nIf we predict for a system\n\nthe maximum delay for packets — every packet is either delivered within some time d or it’s lost.\nthe maximum processing time for service — a non-failed node always handles a request within some time r.\n\nThen the reasonable timeout value is 2d+r. But the prerequisites are impossible: we cannot guarantee any bound for a system, this called unbounded delays.\nPossible Problems\nWhen a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network. If the system is already struggling with high load, declaring nodes dead prematurely can make the problem worse. In particular, it could happen that the node actually wasn’t dead but only slow to respond due to overload; transferring its load to other nodes can cause a cascading failure.\nPrematurely declaring a node dead is problematic: if the node is actually alive and in the middle of performing some action, and another node takes over, the action may end up being performed twice.\n\nUnbounded DelaysThere are 4 network delays:\n\nQueuing Delay &rarr; If several different nodes simultaneously try to send packets to the same destination, the network switch must queue them up and feed them into the destination network link one by one;\nProcessing Delay &rarr; The amount of time it takes processors to process the packet;\nTransmission Delay &rarr; If the request is very large, sender will chop it into several packets, it will take time to put all of these packets into network;\nPropagation Delay &rarr; Time taken for a single bit to traverse the physical medium from one end to the other;\n\nSync Vs AsyncIn sync network, it’s just like the circuit for telephone, we have bounded delays (e.g. known maximum round-trip time, no queuing delays).\n\nEven as data passes through several routers, it does not suffer from queueing, because the 16 bits of space for the call have already been reserved in the next hop of the network.\n\nIn async network, the unbounded delays occur.\nWhy distributed system doesn’t use circuit (sync network) logic? Because distributed system has bursty traffic (we don’t know how many bandwidth should be allocated). A circuit is good for an audio or video call, which needs to transfer a fairly constant number of bits per second for the duration of the call. On the other hand, requesting a web page, sending an email, or transferring a file doesn’t have any particular bandwidth requirement—we just want it to complete as quickly as possible. TCP is good at dynamic allocation, so we choose TCP over circuit.\n\nTCP has traffic and congestion control, it dynamically adapts the rate of data transfer to the available network capacity.\n\n\nUnreliable Physical ClocksClocks and time are import:\n\nHas the request timeout yet?\nWhat’s the 99th percentile response time?\nHow may QPS?\nWhen does the cache expire?\nWhat is the timestamp for logging?\n\nIn a distributed system, time is a tricky business, because communication is not instantaneous: it takes time for a message to travel across the network from one machine to another. What’s more, some machines maybe faster or slower than other machines.\nMonotonic Vs Time-of-Day Clocks\nTime-of-Day &rarr; return the difference with 1970-1-1 00:00:00, the value is meaningful, but due to clock skew in different clusters, the value may not be accurate.\nMonotonic &rarr; the single value is meaningless, but we can get two values then calculate its difference to get the elapsed time, it doesn’t assume any synchronization between different nodes’ clocks and is not sensitive to slight inaccuracies of measurement.\n\nClock Sync and AccuracyClock Sync is a pretty hard task:\n\nThe quartz clock in a computer is not very accurate: it drifts (runs faster or slower than it should).\nIf a computer’s clock differs too much from an NTP server, it may refuse to synchronize, or the local clock will be forcibly reset, which may influence ongoing tasks.\nIf a node is accidentally firewalled off from NTP servers, the misconfiguration may go unnoticed for some time.\nNTP synchronization can only be as good as the network delay.\n…\n\nRelying on Sync ClocksIf you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines. Any node whose clock drifts too far from the others should be declared dead and removed from the cluster.\nLast write wins (LWW)It is widely used in both multi-leader replication and leaderless databases. Its logic is: if there is conflict of multiple writes, keep the write with maximum timestamp, it represents the newest (last) write, and will overwrite old (previous) writes.\nBut it still has some problems:\n\nDatabase writes can mysteriously disappear: a node with a lagging clock is unable to overwrite values previously written by a node with a fast clock until the clock skew between the nodes has elapsed.\nLWW cannot distinguish between writes that occurred sequentially in quick succession or concurrent.\nCausality tracking mechanisms, such as version vectors, are needed in order to prevent violations of causality.\n\nConfidence IntervalThe most common implementation of snapshot isolation requires a monotonically increasing transaction ID. However, when a database is distributed across many machines, potentially in multiple datacenters, a global, monotonically increasing transaction ID (across all partitions) is difficult to generate, because it requires coordination.\nCan we use timestamp? Yes! But it requires materialized design.\nSo instead of treating Time-of-Day value as a precise value, we can treat it as a range of time, just like [minimum possible timestamp, maximum possible timestamp], this is called confidence interval.\nGoogle Spanner use this “confidence interval” concept to implement its distributed transaction semantics, because Spanner depends on Google’s well-designed clock system!\n\nKnowledge, Truth and LiesQuorumA distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes: decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node.\nFencing Tokens\nLet’s assume that every time the lock server grants a lock or lease, it also returns a fencing token, which is a number that increases every time a lock is granted (e.g., incremented by the lock service). We can then require that every time a client sends a write request to the storage service, it must include its current fencing token. This mechanism requires the resource itself to take an active role in checking tokens by rejecting any writes with an older token than one that has already been processed.\nByzantine FaultsByzantine fault &rarr; a node may claim to have received a particular message when in fact it didn’t.\nA system is Byzantine fault-tolerant if it continues to operate correctly even if some of the nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering with the network. This concern is relevant in certain specific circumstances.\nMost Byzantine fault-tolerant algorithms require a supermajority of more than twothirds of the nodes to be functioning correctly.\nSystem ModelSystem model &rarr; formalize the kinds of faults that we expect to happen in a system.\n\nSync\nSynchronous model &rarr; The synchronous model assumes bounded network delay, bounded process pauses, and bounded clock error. This does not imply exactly synchronized clocks or zero network delay; it just means you know that network delay, pauses, and clock drift will never exceed some fixed upper bound.\nPartially synchronous model &rarr; Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift.\nAsynchronous model &rarr; In this model, an algorithm is not allowed to make any timing assumptions—in fact, it does not even have a clock.\nFaults\nCrash-stop faults &rarr; Node may suddenly stop responding at any moment, and thereafter that node is gone forever—it never comes back.\nCrash-recovery faults &rarr; Nodes may crash at any moment, and perhaps start responding again after some unknown time.\nByzantine (arbitrary) faults &rarr; Nodes may do absolutely anything, including trying to trick and deceive other nodes.\n\nSafety &amp; Liveness\nSafety &rarr; Nothing bad happens, for example: transaction with smaller timestamp should happens before transaction with larger timestamp.\nLiveness &rarr; Something good eventually happens.\n\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (9)Linearizability","url":"/2023/11/03/DDIA-cookbook-9-Consistency-and-Consensus/","content":"CAP Theorem\nConsistency &rarr; Every read receives the most recent write or an error\nAvailability &rarr; Every request receives a (non-error) response, without the guarantee that it contains the most recent write\nPartition tolerance &rarr; The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\n\nConsistency GuaranteesMost replicated databases provide at least eventual consistency, which means that if you stop writing to the database and wait for some unspecified length of time, then eventually all read requests will return the same value. The inconsistency is temporary. It’s also called convergence. However, this is a very weak guarantee—it doesn’t say anything about when the replicas will converge. Until the time of convergence, reads could return anything or nothing.\nFor stranger guarantees, they are easy to understand and implement. But they may have worse performance or be less fault-tolerant than systems with weaker guarantees.\n\nDistributed consistency is pretty similar to transaction isolation. But there are some difference: transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.\n\n\nLinearizabilityLinearizability (aka atomic consistency, strong consistency, immediate consistency, or external consistency) &rarr; make a system appear as if there were only one copy of the data, and all operations on it are atomic.\nIn a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written. Maintaining the illusion of a single copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica.\nWhat makes a system linearizable?In a linearizable system, once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again.\n\nIn this example, we imagine that there must be some point in time (between the start and end of the write operation) at which the value of x atomically flips from 0 to 1. Thus, if one client’s read returns the new value 1, all subsequent reads must also return the new value, even if the write operation has not yet completed. Client A is the first to read the new value, 1. Just after A’s read returns, B begins a new read. Since B’s read occurs strictly after A’s read, it must also return 1, even though the write by C is still ongoing.\nA more precise definition for linearizability is: it is possible (though computationally expensive) to test whether a system’s behavior is linearizable by recording the timings of all requests and responses, and checking whether they can be arranged into a valid sequential order.\n\nLinearizability Vs Serializability\n\nSerializability &rarr; an isolation property of transaction, where every transaction may read and write multiple objects (rows, documents, records). It guarantees that transactions behave the same as if they had executed in some serial order. It is okay for that serial order to be different from the order in which transactions were actually run.\nLinearizability &rarr; a recency guarantee on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew.\n\n\nA database that provides both serialiability and linearizability is called strict serializability.\nImplementations of serializability based on 2PL or actual serial execution are typically linearizable.\n\nObviously the serializable snapshot isolation is not linearizable, because it uses snapshot which doesn’t include writes in other transactions that are more recent than the premise.\n\nRelying on linarizabilityWhen is the linearizability useful?\n\nLocking and leader election\nConstraints and uniqueness guarantees &rarr; generating auto-incremental primary key\nCross-channel timing dependencies\nThis is one example for cross-channel. Ideally, after the full-size image being stored in the storage system, we run the message queue task. However, if the storage process is slow, then we may run message queue task first, which tries to resize “existing” full-size image.\n\n\n\nImplementationThe most common approach to making a system fault-tolerant is to use replication.\n\n\n    Single-leader replication (potentially linearizable)\n\n\n    Using the leader for reads relies on the assumption that you know for sure who the leader is. It is quite possible for a node to think that it is the leader, when in fact it is not—and if the delusional leader continues to serve requests, it is likely to violate linearizability. With asynchronous replication, failover may even lose committed writes, which violates both durability and linearizability.\n\n\n\n\n\nMulti-leader replication (not linearizable)\n\n\n    Systems with multi-leader replication are generally not linearizable, because they concurrently process writes on multiple nodes and asynchronously replicate them to other nodes. For this reason, they can produce conflicting writes that require resolution. Such conflicts are an artifact of the lack of a single copy of the data.\n\n\n\n\n\n    Leaderless replication (probably not linearizable)\n\n\n    For systems with leaderless replication, people sometimes claim that you can obtain \"strong consistency\" by requiring quorum reads and writes (w + r > n).\n\n\n    \"Last write wins\" conflict resolution methods based on time-of-day clocks are almost certainly nonlinearizable, because clock timestamps cannot be guaranteed to be consistent with actual event ordering due to clock skew. Sloppy quorums also ruin any chance of linearizability. Even with strict quorums, nonlinearizable behavior is possible.\n\n\n    Why quorums cannot provide linearizability?\n\n\n\n    According to the figure, n = 3, w = 3, r = 2, which meet the quorum requirement. However, this execution is nevertheless not linearizable: B’s request begins after A’s request completes, but B returns the old value.\n\n\n\n\n\n    Consensus algorithms (linearizable)\n\n\n    Some consensus algorithms bear a resemblance to single-leader replication. However, consensus protocols contain measures to prevent split brain and stale replicas. Thanks to these details, consensus algorithms can implement linearizable storage safely.\n\n\n\nCost of linearizability\nA network interruption forcing a choice between linearizability and availability.\n\nImagine that we use single-leader replication, and the network partition split leader with some followers.Because all writes happens in leader, so if we continue to serve client, then some clients that connect to isolated followers will get out-of-date value, which violate the linearizability.On the other hand, if we choose to let the reads wait until the network restart working, then we lose availability.\n\n“Either Consistent or Available when Partitioned”CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. Actually, it’s wrong!\nAt times when the network is working correctly, a system can provide both consistency (linearizability) and total availability. When a network fault occurs, you have to choose between either linearizability or total availability.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (9)Distributed Transactions and Consensus","url":"/2023/11/03/DDIA-cookbook-9-Distributed-Transactions-and-Consensus/","content":"","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (9)Ordering","url":"/2023/11/03/DDIA-cookbook-9-Ordering/","content":"","tags":["System Design","DDIA"]},{"title":"Friend function inside template class","url":"/2023/08/09/Friend-function-inside-template-class/","content":"Assume we have a template class named Matrix, and we want to add a friend function called swap().\nMethod 1template&lt;typename T&gt;class Matrix &#123;    // ...    friend void swap(Matrix&lt;T&gt;&amp; a, Matrix&lt;T&gt;&amp; b) &#123;        // definition    &#125;&#125;;\n\nNotice, the swap() is not a template function! It’s just a normal function.\nWhy we need to define it inside the class?\n\nBecause when we create a Matrix&lt;int&gt;, it will also define a void swap(Matrix&lt;int&gt;&amp;, Matrix&lt;int&gt;&amp;) for system; for Matrix&lt;double&gt;, it’s the same. In other word, when we create certain type Matrix, the swap which supports that certain type will be automatically created for us. So that when we use swap(), it always make sure we can find the definition.\n\nCan we define it outside the class?\n\nYes, you can, but you need to make sure you write definitions for all possible types, like you need to implement a void swap(Matrix&lt;int&gt;&amp;, Matrix&lt;int&gt;), a void swap(Matrix&lt;double&gt;&amp;, Matrix&lt;double&gt;), etc for all possible typename T.\n\nMethod 2template&lt;typename T&gt;class Matrix;template&lt;typename T&gt;void swap(Matrix&lt;T&gt;&amp;, Matrix&lt;T&gt;&amp;);  // we need to know what is Matrix&lt;T&gt;, so declare Matrix beforetemplate&lt;typename T&gt;class Matrix &#123;    // ...    friend void swap&lt;&gt;(Matrix&amp; a, Matrix&amp; b);   // we need to know that is a template function, so declare before&#125;;template&lt;typename T&gt;void swap(Matrix&lt;T&gt;&amp; a, Matrix&lt;T&gt;&amp; b) &#123;    // definition&#125;\n\nNotice here the swap is a template function, so that we need to write it as swap&lt;&gt;, and we don’t need to explicitly denote Matrix type, cuz template funtion can deduce its type.\nCan we don’t write the declarition of template swap?\n\nNo. Inside class definition, we tell the class that we have a template friend function. If we don’t declare it before class definition, the class don’t know how many typenames it has, do it have default typename, etc.\n\nTricktemplate&lt;typename T&gt; void helper(T);template&lt;typename T&gt;class Obj &#123;    // ...    friend void helper&lt;char&gt;(char); // no matter what type Obj is, only helper with char type can get access to private members&#125;\n","tags":["c++"]},{"title":"Unittest in python - Mock","url":"/2023/07/27/Unittest-in-python-Mock/","content":"IntroMock is a very useful package for unittest in python. It can replace some classes or functions and change their behaviors, it can also use some built-in methods to help you assert whether pytest calls certain part of your code.\n\nMock() &amp; MagicMock()\nFor simplicity, let’s use Mock for example. In most cases, Mock and MagicMock are the same :)\n\nMock is a class that create all attributes and methods as you access them and store details of how they have been used.\nWhat’s more, you can set anything to a Mock, it will treat them as new Mock (sub Mock).\n# set an undefined method to a Mockm = Mock()              # &lt;Mock name=&quot;mock&quot;&gt;m.undefined_function()  # &lt;Mock name=&quot;mock.undefined_function()&quot;&gt;# use mock as a argumentclass Object:    def func(self, args):        args.do_something()o = Object()m = Mock()o.func(m)m.do_something()    # &lt;Mock name=&quot;mock.do_something()&quot;&gt;\n\nreturn_valueBy setting some methods or functions as Mock, then setting return_value can change original logic: I don’t care about what you write in the function, just return what I want!\nclass Object:    def __init__(self, x):        self.x = x        def func(self):        return self.xobj = Object(1)# method 1obj.func = Mock(return_value = 1024)# # method 2# obj.func = Mock()# obj.func.return_value = 1024obj.func()  # return 1024 rather than 1\n\nThis is always useful in unittest, like:\n\nI don’t want to send a real request via network, just let the requester &#x2F; dispatcher return what I want;\nI don’t want to access a real DB, just tell me what data you have;\n\nclass MySvc:    def __init__(self, db):        self.db = db        ...    def myRequest(self, req):        ...        results = self.db.fetch(req)        return resultsdef test_db(req):    db = MyDB()    db.fetch = Mock(return_value = [data1, data2, data3, ...])    svc = MySvc(db)    results = svc.myRequest(req)    # [data1, data2, data3, ...]\n\nSometimes, we will meet some call chains, such as mock.connection.cursor().execute(...).\n# mock.call1().call2().call3()m = Mock()# get Mock for all calls except the last onec1 = m.call1.return_valuec2 = c1.call2.return_value# set Mock for last one callc2.call3.return_value = &quot;foo&quot;m.call1().call2().call3()   # &quot;foo&quot;\n\n\nBasically, we can change the code as following\nm = Mock()c1 = Mock()m.call1.return_value = c1c2 = Mock()c1.call2.return_value = c2c2.call3.return_value = &quot;foo&quot;\n\nside_effectside_effect &#x3D; Exception&gt;&gt; m = Mock()&gt;&gt; m.exception_side_effect = Mock(side_effect = ValueError)&gt;&gt; m.exception_side_effect()ValueError\n\nside_effect &#x3D; iterableIf we set iterable to side_effect, every time we call it, it will yield one element.\n&gt;&gt; m = Mock()&gt;&gt; m.iter = Mock(side_effect = [1, 2, 3])&gt;&gt; m.iter()1&gt;&gt; m.iter()2&gt;&gt; m.iter()3\n\nside_effect &#x3D; callabledef log(*args, **kwargs):    print(f&quot;args: &#123;args&#125;, kwargs: &#123;kwargs&#125;&quot;)&gt;&gt; m = Mock()&gt;&gt; m.func = Mock(side_effect = log)&gt;&gt; m.func()args: (), kwargs: &#123;&#125;&gt;&gt; m.func(1, two = 2)args: (1,), kwargs: &#123;&quot;two&quot;: 2&#125;\n\nWhen we set both return_value and side_effect, the Mock will only use side_effect!!\nspec &amp; spec_setspec can be either a list of string or an existing class &#x2F; instance. After we set spec, the mock can only have corresponding attributes and methods (just like we use dir to see what attributes and methods does one class support).\nclass Object:    def __init__(self):        self.one = 1        self.two = 2    def func(self):        pass# when we use existing class as spec, the mock hasn&#x27;t initedm = Mock(spec = Object)m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # error, cuz we don&#x27;t init the Objectm.__init__()m.one       # 1m.three = 3 # ok# when we use existing instance as spec, the mock has initedo = Object()m = Mock(spec = o)m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # 1# when we use list of string as specm = Mock(spec = [&quot;one&quot;, &quot;func&quot;])m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # &lt;Mock name=&quot;mock.one&quot;&gt;m.one()     # &lt;Mock name=&quot;mock.one()&quot;&gt;\n\nThe difference between spec and spec_set is, spec can add new stuff while spec_set can only read.\nm = Mock(spec = [&quot;one&quot;])m.onem.two = 2   # okmm = Mock(spec_set = [&quot;one&quot;])m.onem.two = 2   # error\n\nassertion &amp; call argsMock supports lots of assertions, such as assert_called, assert_called_once, assert_called_with, etc.\nm = Mock()m(1, 2)m.assert_called()           # Truem.assert_called_with(1, 2)  # True\n\nMock can also remember what args you used via call_args or call_args_list.\nm = Mock()m(1, 2)m.call_args         # call(1, 2)m.call_args_list    # [call(1, 2)]m(3, 4)m.call_args         # call(3, 4)m.call_args_list    # [call(1, 2), call(3, 4)]\n\nWhat’s the difference between these two?So you can simply think MagicMock &#x3D; Mock with pre-defined magic methods.\n&gt;&gt; len(Mock())Error, Mock doesn&#x27;t have __len__ method&gt;&gt; len(MagicMock())0\n\nSo if you want to test or use magic methods in your test, use MagicMock.\nIf you want to modify the magic methods or just for simplicity purpose, plz use Mock.\n","tags":["python","test","pytest"]},{"title":"Unittest in python - Patch","url":"/2023/07/27/Unittest-in-python-Patch/","content":"After introduced mock in previous section, let’s talk about another powerful module – patch.\nIntropatch is a decorator &#x2F; context manager, and it can help you use some new stuff (default is Mock()) to replace target.\ndirectly use# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patchdef main():    mock_func = patch(&quot;demo.func&quot;)    mock_func.return_value = 10    mock_func.start()    demo.func()     # 10    mock_func.end()    demo.func()     # 1\n\ncontext manager# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patchdef main():    with patch(&quot;demo.func&quot;) as mock_func:        mock_func.return_value = 10        demo.func() # 10    demo.func()     # 1\n\ndecorator# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patch@patch(&quot;demo.func&quot;)def test_main(mock_func):    mock_func.return_value = 10    demo.func() # 10\n\nnotice 0Please notice, if we have multiple @patch for one function, the order is important: the inner decorator decorates front parameter.\n@patch(&quot;demo.func2&quot;)@patch(&quot;demo.func1&quot;)def test(mock_func1, mock_func2):    ...\n\nnotice 1Decorator can also be used in class. Remember patch is used for test, so\n\nonly function name starts with test_ will be treat as test function\nonly class derives from unittest.TestCase will be treat as test class\n\n@patch(&quot;demo.func&quot;)class MyTest(unittest.TestCase):    def test_1(self, mock_func):        ...    def test_2(self, mock_func):        ...        # patch is not working    def mytest_func(self):        ...\n\n\npatchunittest.mock.patch(target, new=DEFAULT, spec=None, create=False, spec_set=None, autospec=None, new_callable=None, **kwargs) \n\ntarget: target object’s path, remember it must be a string looks like package.module.className. If the object is defined in the same file, please use __main__.className.\nnew: default is MagicMock(), it can be a value or a actual object.\nnew_callable: it is a callable to create object.\nspec &amp; spec_set: please refer Mock parts\n\ndef new_func():    return 10def main():    mock_func = patch(&quot;demo.func&quot;, new = new_func)    mock_func.start()    demo.func()     # new_func() -&gt; 10    mock_func.end()\n\npatch target pathIf we want to patch some function, the path is not where we define the function, is where we use it.\n# package2.m2.pyfrom package1.m1 import func1def func2():    func1()\n\n# test.py@patch(&quot;package2.m2.func1&quot;)\n\nnew vs return_valuedef main():    mock_func = patch(&quot;demo.func&quot;, return_value = 10)    mock_func.start()    demo.func() # 10    mock_func.end()def main():    mock_func = patch(&quot;demo.func&quot;, new = 10)    mock_func.start()    demo.func   # 10    mock_func.end()\n\nnew vs new_callablenew is an instance, new_callable is a callable to create instance.\nfoo = 10with patch(&quot;__main__.foo&quot;, new = 100):    foowith patch(&quot;__main__.foo&quot;, new_callable = lambda: 100):    foo\n\ndef return_100():    return 100def main():    mock_func = patch(&quot;demo.func&quot;, new = return_100)    mock_func.start()    demo.func() # 100    mock_func.end()# don&#x27;t recommend to assign a value to newdef main():    mock_func = patch(&quot;demo.func&quot;, new_callable = return_100)    mock_func.start()    demo.func   # 100    mock_func.end()\n\nCannot use ‘new’ and ‘new_callable’ together!\npatch(&quot;demo.func&quot;, new = xxx, new_callable = xxx)   # error\n\nhow to patch a whole class# util.pyclass Object:    def __init__(self, x, y):        self.x = x        self.y = y        def show(self):        print(f&quot;x = &#123;x&#125;, y = &#123;y&#125;&quot;)        return 0\n\n# keep this the same as __init__def constructor(self, x, y):    self = Mock(spec = Object)    self.x = 2 * x    self.y = 2 * y    self.show.return_value = 100    return self@patch(&quot;util.Object&quot;, new = constructor)def test_patch_with_new():    o = Object(10, 10)    res = o.show()      # x = 20, y = 20    print(res)          # 100    o.x = 1000    print(o.x)          # 1000\n\n\npatch.objectUsed to mock methods in one class.\nimport Object# only mock func inside Object class@patch.object(Object, &quot;func&quot;)def test_patch_object(mock_func):    mock_func.return_value = 100    o = Object()    o.func()    # 100\n\n\n\npatch.dictm = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;with patch.dict(m, &#123;&quot;a&quot;: 10, &quot;b&quot;: 20&#125;, clear=True):    m[&quot;a&quot;]  # 10\n","tags":["python","test","pytest"]},{"title":"What is .pyi file","url":"/2023/09/12/What-is-pyi-file/","content":"Sometimes in VSCode, we want to “jump to declaration”, then IDE redirect us to a file with suffix .pyi, and all it contains are some strange code like this:\nclass Object:    def __init__(self, name: str, age: _T) -&gt; None:        ...        def get_name(self) -&gt; str:        ...        def get_age(self) -&gt; _T:        ...        def __lt__(self, other: Object) -&gt; bool:        ...        def __add__(self, v: _T) -&gt; _T:        ...\n\nIt looks like .h file in C++, only declares the function signature, typedef, etc without any implementation.\nActually, a .pyi file is not required but recommended, the only purpose for .pyi file is: IDE can provide auto complete and programmer can read the APIs fast and don’t need to care about the implementation details.\n","tags":["python"]},{"title":"Why and why not inline","url":"/2023/07/20/Why-and-why-not-inline/","content":"Why inlineIn C++, we can add inline keyword in front of function defination (inline only works on defination, you don’t need to use inline in statement).\nvoid this_is_inline_func();inline void this_is_inline_func() &#123;    std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;&#125;\n\nno inlineWhen we call a function, the process is\n\nfind function defination and its address\nload from memory to stack\nexecute\npop from stack\n\nvoid this_is_not_inline() &#123;    ...&#125;this_is_not_inline();   // 1this_is_not_inline();   // 2this_is_not_inline();   // 3\n\n0x0000 -&gt; defination of this_is_not_inline...0x0100 -&gt; call 1    // find address is 0x0000 -&gt; push 0x0000 into stack -&gt; execute -&gt; pop0x0104 -&gt; call 2    // same0x0108 -&gt; call 3    // same\n\ninlineAll of these steps will cost extra time. To avoid this cost, we can use inline. After we defining a function as inline, we may have multiple calls in code. When we compile the code, the compiler will change function calls to function defination, so that when execute the code, instead of wasting time finding defination and loading from memory to stack, it just executes the function logic.\nvoid this_is_inline() &#123;    std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;&#125;this_is_inline();   // 1this_is_inline();   // 2this_is_inline();   // 3\n\n0x0000 -&gt; defination of this_is_inline...0x0100 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;  // change call to defination0x0200 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;0x0300 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;\n\n\nWhy not inlineSounds great? But the problem is, we need to copy the defination after every calls. If the defination is very complicated, we will cost losts of memory. What’s more, if the function is complicated, the execution time &gt;&gt; load &amp; pop stack time, we can ignore the inline improvement.\n\nSuggestionOnly use inline for very simple function. Actually inline is just a recommendation rather than requirement. If the function contains loop, recursion, static, etc complex logic, the function will be treated as not-inline even if you add inline keyword.\nWhat’s more, in class, the methods defination will be treated as inline. So only define simple methods in class, for complex methods, leave statements inside class, then define them outside class.\nclass Object &#123;public:    Object() =default;    Object(const Object&amp; rhs) &#123; num = rhs.num; &#125;    int get_num() &#123; return num; &#125;   // define simple method inside class (auto inline)    void inline_function();    void complex_function();private:    int num;&#125;;// we can still define inline outside classinline void Object::inline_function() &#123;    ...&#125;// for complex method, plz define outside classvoid Object::complex_function() &#123;    ...&#125;\n\n\n#define vs inline#define replaces the pure text in pre-complie stage, inline replaces function call with function defination in complie stage.\n","tags":["c++"]},{"title":"#include <> Vs #include \"\" ","url":"/2023/07/16/cpp_include_difference/","content":"#include &lt;header&gt;This will let preprocessor find header files in pre-designated directories.\n\nThese directories are normally system-related, such as “&#x2F;usr&#x2F;include”, “&#x2F;x86&#x2F;include”, etc.\n\nLet’s use Linux as example. If we ls /usr, you will see bin, include and lib, and if we ls /usr/include, output will contain lots of system defined header files, such as /usr/include/cpp/vector and /usr/include/cpp/iostream. This is how we include these commonly used packages.\n// find header file named &quot;iostream&quot; from &quot;/usr/include&quot;#include &lt;iostream&gt;\n\n\n#include &quot;header&quot;This will search programmer-defined header files and typically includes same directory as the file containing the directive.\nmy_project| - include|   | - my_header.h| - my_project.cpp\n// my_project.cpp#include &quot;my_header.h&quot;\n\nLet’s assume the project tree looks like that, and when we #include &quot;&quot;, it will search header files under /my_project directory.\nIf the processor cannot find header file in current project directory, it will search header files in system path (the same as #include &lt;&gt;).\n\n#include &quot;header.h&quot; – cannot find locally –&gt; #include &lt;header.h&gt;\n\n\nAdd include pathAs we discussed before, #include &lt;&gt; will search pre-designated directories. What if we want the processor to search other directories?\nWe can add include paths via \n\n-I\nsystem environment variable\nCMake\n\nAfter adding include paths, the searching order is\n\n#include &lt;&gt;\ndir list\nsystem dir\n\n\n#include &quot;&quot;\nlocal\ndir list\nsystem dir\n\n\n\n-IWe can use -I flag in command line to add search paths.\ng++ -c test.cpp -o testg++ -c test.cpp -o test -I /path/to/certain/include/\n\nSystem environment variableThe system environment has a property called CPLUS_INCLUDE_PATH, we can set this to add search paths.\nexport CPLUS_INCLUDE_PATH=&quot;path/to/certain/include&quot;\n\nCMakeFor CMake, we can use include_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...]) .\n","tags":["c++"]},{"title":"How to setup blog?","url":"/2023/07/14/how_to_setup_blog/","content":"GithubJust create a new repo, and the repo’s name follows rule “username.github.io”.\n\nNode.js &amp; HexoInstallJust donwload Node.js from offical website (maybe you also need npm :)). For Mac user, the simplest way is run command:\nbrew install node\n\nFor Hexo, we just need to run\nnpm install hexo-cli -g\n\nAfter installation, use commands to validate:\nnode -vnpm -vhexo -v\n\nInitIf everything is OK, run following commands:\nmkdir blog &amp;&amp; cd bloghexo init   # init the folder as blog reponpm install # install necessary node_modules\n\nThen you will find a file named blog/_config.yml, add these info in last rows:\n# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy:  type: git  repository: &lt;your github repo&gt;  branch: master\n\nDeployYou can add a new plug-in npm i hexo-deployer-git so that we can post blogs to git via hexo.\nhexo new post &quot;this is post name&quot;hexo g  # generatehexo s  # can view changes in localhosthexo d  # deploy to github\n\n\n:money:\nActually this step is optional, cause u can use “username.github.io” to browse your blog. However, if you want a fancy URI (and you are rich), you can follow the step.\n\nYou should buy a web URI, I got one from GoDaddy.\nAfter you got one, let’s say it’s “myfancyblog.com”, you should set the DNS so that when other people enter “myfancyblog.com” in browser, it can re-direct to “username.github.io”.\nOther people                  Your github repo&quot;myfancyblog.com&quot;   - DNS -&gt;  &quot;username.github.io&quot;\n\nIn DNS Management page, you should set two more DNS Records.\n\n\n\nType\nName\nData\n\n\n\nA\n@\nIP Address For username.github.io\n\n\nCNAME\nwww\nusername.github.io\n\n\nThe first one re-directs “myfancyblog.com” to “username.github.io”‘s IP, the second re-directs “www.myfancyblog.com“ to “username.github.io”.\nThen, you should go back to your blog repo, in blog/source create a new file CNAME, then put “myfancyblog.com” in this file. hexo g &amp;&amp; hexo d. You will see github.com will have a new CNAME file, and you can check “Setting -&gt; Pages”, it should have “myfancyblog.com” in “Custom domain” field.\n\nTipsIf you are curious about “why I changed some settings but the web page seems no change”, :(, well, remember to clear the browser cache.\n","tags":["blog","tools"]},{"title":"tmux cookbook","url":"/2023/07/17/tmux-cookbook/","content":"TmuxWhen we use command line tools (CLI), we will open a terminal, and input some commands. This process called “session”. But session is temporary, which means when we close the terminal, the session ends.\nHow can we keep the session even we close the page?\nUsing Tmux :)\nTmux will create a new terminal (you can think this is a sub-terminal), and we can run commands inside. When we close the terminal, it will not end the session, just return back to the main terminal, we can connect the sub-terminal again via tmux.\nWhen use tmux?This is a difficult question. For me, I will use tmux in 2 situations:\n\nI want to show off. You know what I mean, multiple panes in one screen, especially use one pane to run top command, it’s just like hack style.\nThe task will take a long running time. For instance, the machine learning stuff, you can run it in one session, then go back to working. After one million year, re-connect the session and see error information like “No Module Named xxx” (just don’t ask me why I know that)\n\ncookbook\n\n\nCommand\nFunctionality\n\n\n\ntmux new -s &lt;name&gt;\ncreate a named session\n\n\ntmux detach\nkeep the session and go back to main terminal\n\n\ntmux ls\nlist sessions\n\n\ntmux attach -t &lt;name&gt;\nre-connect to session\n\n\ntmux kill-session -t &lt;name&gt;\nend session\n\n\ntmux switch -t &lt;name&gt;\nswitch to session\n\n\ntmux split-window\nsplit up &amp; down\n\n\ntmux split-window -h\nsplit left &amp; right\n\n\ntmux select-pane -U&#x2F;D&#x2F;L&#x2F;R\nselect up&#x2F;down&#x2F;left&#x2F;right pane\n\n\ntmux swap-pane -U&#x2F;D\nchange pane’s size\n\n\nhot key\n\n\nhot key\nfunctionality\n\n\n\nctrl+d\nexit and kill session\n\n\nctrl+b d\ntmux detach\n\n\nctrl+b ?\nhelp\n\n\nctrl+b s\ncheck all sessions\n\n\nctrl+b $\nrename session\n\n\nctrl+b %\nsplit left &amp; right\n\n\nctrl+b “\nsplit up &amp; down\n\n\nctrl+b x\nkill session\n\n\nctrl+b &lt;arrow key&gt;\nselect pane\n\n\nctrl+b ctrl+&lt;arrow key&gt;\nresize pane\n\n\nctrl+b !\nmake panes into different sessions\n\n\nctrl+b :set &lt;command&gt;\nset properties, eg: :set mouse on to enable mouse control\n\n\n","tags":["tools","cookbook"]}]