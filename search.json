[{"title":"DDIA cookbook - (1)Reliable, Scalable, and Maintainable Application","url":"/2023/08/10/DDIA-cookbook-1-Reliable-Scalable-and-Maintainable-Application/","content":"IntroCPU is no longer the bottle neck. New problems are amount of data, complexity of data and the speed of changing. We call it data-intensive.\n\nStorage - Database\nSpeed - Cache\nSearch - Indexing\nUnkonwn size, continuous, asynchronous - Stream Processing\nAccumulated data - Batch Processing\n\nRight now, single tool is hard to meet requirements. And new tools are designed to optimize for variety of use cases, the boundary between category is blurred.\n\nReliabilityWhat is correct? It’s hard to define, but we can simply consider:\n\nThe App performs what user expected\nThe App can tolerate some mistakes or using it in unexpected ways\nThe App is good enough under certain load and data volume\nThe App prevents any abuse or unauthorized access\n\nThen we can define reliable means “continuing to work correctly, even bad things happend”.\nThe bad things are called faults, a reliable system should be fault-tolerant.\n\nfault vs failure\n\nfault - system deviates from original design\nfailure - system cannot work (crush)\n\nWe should design fault-tolerance mechanisms to prevent faults from causing failure\n\nHardware FaultsMorden hardware system will use RAID to add redundancy to reduce the fault rate.\nHardware faults are random and independent for most of the cases.\nSoftware ErrorsSoftware errors are sometimes correlated. And these bugs may hide for a long time until we trigger it.\n\nScalabilityScalability is used to describe a system’s ability to cope with increased load or changed resources.\nLoadRemember use case is always the key. Load can be:\n\nrequest per second\nread &#x2F; write ratio\nhit rate on cache\n…\n\nWhen we consider the load, the first thing is make the use case clear. There isn’t best solution, there is only suitable solution.\nPerformanceThere have two situations:\n\nIf load increases, and we keep the resource unchanged, how the performance changes?\nIf load increases, how many resources we need to change to keep the performance unchanged?\n\nTo solve these, we need to measure performance.\nThere are two key term: \n\nthroughput &rarr; the number of tasks we can process per second\nresponse time &rarr; the time between client sending request and receiving response\n\n\nlatency vs response time\n\nlatency &rarr; duration for a request waitting to be handled\nresponse time &rarr; user aspect, I send a request, how long it takes until I get response, it may include network delay, queuing delay, processing time, etc\n\n\nPercentileIf we run a request multiple times, the response time is not a fixed number, it has distribution. So average response time is p50 (50% percent).\nFor most of the response time, it looks good, so we always pay attention to tail latencies (high percentile), like p99 (90%) or p999 (99.9%). These response time is always very large and affect user’s experience.\nSLO SLAService Level Objectives &amp; Service Level Agreements are contracts that define the expected performance and avaliability of a service.\nFor example, some SLAs may define “p50 &lt; 50ms, p99 &lt; 100ms”.\nQueuing Delay &amp; HoLQueuing delay is one of the most significant reason for tail latency, because limited resource can only handle limited things in parallel.\nIf we have many requests, they will form a queue. Even the following 99% requests are fast, if the first 1% requests are slow, it will block the queue, and make the total execution time increasing. It’s called Head-of-Line block.\nApproaches for coping with loadScale up &rarr; vertical scale, means build more powerful machine\nScale out &rarr; horizontal scale, means distribute total load into several small machines\nElastic &rarr; autoscale, means this system can detect load changing, and automatically scale to keep performance\n\nMaintainability\nOperability - make it easy for operation team to keep the system running smoothly\nSimplicity - make new engineer can understand the system easily\nEvolvability - make it easy for adding new features\n\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (3)B-Tree and LSM-Tree","url":"/2023/08/17/DDIA-cookbook-3-B-Tree-and-LSM-Tree/","content":"LogHere, we define log as an append-only sequence of records.\n\nIndexIndex is an additional data structure, which doesn’t affect the contents of database, but affect the operation performance.\nIndex can speed up the read, but slow down the write cuz we need to maintain the index.\nHash IndexKey-Value StoreHash index is commonly used in Key-Value Storage. We can keep an in-memory hash map where every key is mapped to a offset in the data file. And we also store the k-v in log file which locates in disk. Every time we write a new k-v pair, we append the k-v in log file (notice log file in disk), and modify the index in memory. \nHowever, it may run out of disk space. A good way to solve this problem is Compaction.\nCompaction\nWe can store the index information in the segment, when the segment reach its size limitation, we make subsequent write in new segment. Then we can perform compaction process for these frozen segments. If same key appears many times in these segment, we just keep the newest value. It will generate the new compacted segments in new files and delete old segments to save space.\nThe whole process can happen in background thread, so it will not affect the service.\nTo find a value for a key, we just check the most recent segment, if the key is not present, then second-most-recent and so on.\nConsiderations\nDeleting Records &rarr; we need to keep the delete operation in logs (sometimes use special mark called tombstone)\nto mark the pervious operations for this key are useless, so if there is no new record for this key, this key shouldn’t appear in compacted segment.\nCrash Recovery &rarr; when database is restarted, in-memory hashmap is erased. We can iter all segment k-v info to rebuild the hashmap, but it’s costy. We can choose to store the k-v info and snapshots of the in-memory hashmap on disk, so that we just load the snapshots to rebuild hashmap.\nConcurrency Control &rarr; as writes are appended to the log in a strictly sequential order, a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by multiple threads.\n\nlog-structured vs update-in-placelog-structured &gt; update-in-place\n\nappend (sequential access) is always more efficient than random access\nconcurrency control and crash recovery are simpler &rarr; thanks for immutable log (or say it’s append-only)\ncompaction can reduce fragment problem\n\nlog-structured &lt; update-in-place\n\nkey must in memory\nkey is good for point searching but is bad for range searching\n\nSSTable &amp; LSM-TreeIf we let the key are sorted in segment, we call it Sorted String Table or SSTable.\nWhy SSTable is useful?\n\nMerging&#x2F;Compacting segments is simple and efficient. It use merge sort algorithm to achieve high efficiency. If one key appears in multiply segments, just use the most up-to-date segment value.\n\n\n\nSparse index. We don’t need to keep all keys in memory, instead, just keep sparse index.\nFor example, create key for “A”, and key for “C”, then key “B” must between these two.\n\n\nCompression. Since read requests need to scan over several key-value pairs in the requested range, it’s possible to group those records into a block and compress it before writing it to disk.\n\nConstruct and maintain SSTable\nWhen write comes in, add it to a in-memory data structure (red-black tree, AVL tree, etc). This in-memory tree is called memtable.\nThis data structure can insert unordered data, and dump ordered data, for simplicity, just imagine binary search tree, we can insert data in any order, then read them via pre-order traversal to get ordered output.\n\n\nWhen the memtable reachs its threshold, dump it into disk as a new SSTable (the data structure ensures when dump, it must be ordered). After SSTable is being written in disk, writes can continue to a new memtable.\nWhen read comes in, it check memtable first, then the most up-to-date SSTable, and so on.\nFrom time to time, a background process is running for compacting.\n\nThe only problem is: if crash happens, the memtable will lose all fresh data. To deal with it, we can add a log file in disk for memtable, if crash happens, just recover it from log file; if memtable dumps to disk, then delete the log file cuz it’s useless.\nMaking LSM-TreeLog-Structured Merge Tree is based on SSTable and memtable priciple. It will compact SSTable according to level or size.\n\nOptimization\nBloom Filter to aviod not exist keys.\nsize-tiered and level-tiered compaction.\n\n\nBloom Filter &rarr; it can judege that an element must not exist or possible exist\nFor a given input x, apply multiple hash functions to map its output in different place (you can imagine we have a vector&lt;bool&gt;), e.g. hash1(x), hash2(x), hash3(x). Then when we have a new input y, if hash1(y), hash2(y) and hash3(y) all have value (true in vector), which means y is possible exist; if any of these output don’t have value (false in vector), which means y must miss.\n\nThe basic idea of LSM Tree is: keeping a cascade of ordered SSTables that are merged in the background.\nBecause the data is sorted and sparse index can narrow the possible range, range query is also efficient, cuz you can use like binary search to boost query speed.\nB-TreeDifferent with LSM Tree who uses variable size of segments, B Tree breaks the database down into fixed size block or page, which aligns with disk design. So for any operations, the B-Tree uses page as unit.\nFor read, it’s just like find a value in binary search tree, whereas here is N-ary search tree, and the N called branching factor.\nFor update, it needs to load the whole page, update, then write the whole page back.\nFor write and delete, it may need to split &#x2F; merge node to make sure the tree is balanced.\nMaking tree reliableFor LSM Tree, the modifications will not change data in-place, instead, it will append new data and write to new place when compaction happens. But for B Tree, it wants to modify data in-place, which means modification doesn’t change the location of page.\nWhen the tree structure is adjusted, many pages may be modified in cascade. For example, after a leaf node is split, two new leaf nodes and a parent node need to be written (updating the leaf pointer).\n\ncrash &rarr; WAL (write ahead log), which means write the operation in log before execute it\nconcurrency control &rarr; latch (lighweight lock)\n\nOptimization\nInstead of WAL, use Copy-On-Write mechanism &rarr; instead of write page back to original place, create a new page in new place and modify parent’s pointer\nInstead of keep entire key, we can share prefix &rarr; for example, if the key is YYYYMMDD, the parent keeps YYYY, then its children keep MM, then DD. When we query, we must start from root, which means from left to right for key\nAdd pointers between leaf nodes to boost range query\n\nLSM-Tree vs B-TreeWrite Amplification &rarr; one write in database resulting in multiple writes to the disk.\nFor SSD, if there is a “delete” for some data, system cannot delete it immediately, system will give it a marker to indicate it’s useless. Remember for disk, the operation unit is page (or you can imagine it’s an area which contains many data), so when we execute a write:\n\nmove all useful data out of the page, and store them in somewhere\nerase all data in this page\nmove useful data back and add new data\n\nIt’s obviously writing more data than what we actually want, and this is the amplification.\n| | B-Tree | LSM-Tree || — | — | — | — || R&#x2F;W | read faster | write faster || Write Amplification | 1. data + WAL  2. massive data may cover different pages | 1. data + WAL  2. compaction needs to write in a new file || Write Throughput | low &rarr; random access | high &rarr; lower amplification; sequential access; compression makes smaller SSTable || Storage performance | lots of fragments | compaction save space || Bandwidth | predictable | compaction will take some bandwidth, which may infer service || Storage Amplification | some pages have unused space due to alignment | the same key exist in multiple segments || Concurrency | can add latch in parent nodes | one key exist in many places &rarr; MVCC |\nOther IndexPrimary vs Secondary| Id | Name | Age || --------------- || 1  | Tom  | 18  || 2  | Bob  | 18  || ...             |\n\nPrimary Index &rarr; index key is unique (such as primary key in MySQL), such as idSecondary Index &rarr; index key may be mapped to several values, such as age\nCluster vs Non-Cluster| Id | Name | Age || --------------- || 1  | Tom  | 18  || 2  | Bob  | 18  || ...             |\n\nCluster &rarr; index value is data, such as index[id=1] -&gt; &#123;id: 1, name: &quot;Tom&quot;, age: 18&#125;Non-Cluster &rarr; index value is pointer of data, such as index[age=18] -&gt; [id=1, id=2]\n\nFor MySQL\n\nprimary index is always cluster index &rarr; the leaf node of B-Tree is &lt;primary_key, row&gt;\nsecondary index is always non-cluster index &rarr; the leaf node of B-Tree is &lt;secondary_index, primary_key&gt;\n\n\n\nIn-Memory DBThe basic property for memory is: its data will lose after we restart the system. (except Non-Volatile Memory, aka NVM)\nWe can categorize in-memory DB into two categories:\n\nCache only &rarr; no persistency\nPersistency &rarr; use WAL, snapshot, replica for recovery and reload, but execute all operations in memory\n\nWhy in-memory DB is so efficient? Many people may think the biggest reason is it doesn’t need to communicate with disk. However, the truth is in-memory DB doesn’t need to decode&#x2F;encode data structures to fit into disk.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (2)Data Models and Query Languages","url":"/2023/08/10/DDIA-cookbook-2-Data-Models-and-Query-Languages/","content":"Data ModelA data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities.\nMorden applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below it by providing a data model.\n\nRelational Vs Document ModelOne-to-Many\nFor relational model, is hard to represent One-to-Many relationship, like one person may have 0 to infinite work experience.\n\nRelational &rarr; create multiple table to store company info, school info, then use foreign key to JOIN several tables\nDocument &rarr; can use JSON-like structure, easy to read by human, and better locality (store these info in one place)\n\nMany-to-One &amp; Many-to-ManyWhen we store the region, we use ID rather than pure text. This is because ID has no meaning, it never needs to change. For example, if we want to update “Greater Seattle Area” to “Seattle”, we just need to modify the text in region_table.\nDocument model is good at One-to-Many because you can imagine it as a tree, but it’s not good at Many-to-X, because it looks like a graph.\nIf document model doesn’t support JOIN, then we need to use iteration to mock JOIN in application level. Even if it supports JOIN, we still need to use document reference (just like foreign key), which is similar to relational model.\nlocalityFor document database, it always store the whole document as a single object.\nFor read, it need to load the whole document from disk to memory, so if we need most of parts inside the document, it’s fine; otherwise, its performance is poor.\nFor write, it also need to rewrite the whole document from memory to disk, and only modifications that don’t change the total encoded size of a document can easily be performed in place; otherwise, system need to assign new space for new document.\nschema-on-read vs schema-on-writeDocument Database is not schemaless. Actually, it has implicit constrain, like when we write service code to read something from DB, we assume we can get some fields, so schema-on-read is a more accurate term.\n\n\n    schema-on-read\n    check when we READ\n    poor efficiency, cuz we cannot do any optimizations when we write it\n\n\n    schema-on-write\n    check when we WRITE\n    good efficiency cuz we can check the type then do optimization\n\n\n\nSummary\n\n\n\ndocument\nrelation\n\n\n\nrelation map\ntree, one-to-many\ncan use foreign key to achieve many-to-X\n\n\nJOIN\n:(\n:)\n\n\nflexibility\nflexible, can add fields easily\nschema, hard to change\n\n\nlocality\nif operate the whole doc, performance is good; but if only operate partial doc, performance is not good\nscatter in tables\n\n\n\nQuery LanguageDeclarative vs Imperative Language\n\n\n\nDeclarative\nImperative\n\n\n\nConcept\ndeclare the logic rather than actual execution\ndefine the execution plan\n\n\nExample\nSQL, CSS\nC++, Python, …\n\n\nAbstraction\nhigh\nlow\n\n\nParallel\ngood, cuz we let system do the optimization\npoor, cuz we already defined the steps\n\n\n\nWhat u want?\nHow to do that?\n\n\nHere are some advantages for declarative language:\n\nMore concise and easily use\nHide implementation details\nGood support for parallelism\n\nMapReduce QueryMapReduce is neither a declarative language nor a imperative language.\n\n\ndeclarative &rarr; we don’t need to specify how to iter or shuffle dataset\nimperative &rarr; we need to implement map and reduce functions\n\n\nIt requires the map and reduce are pure function, which means they only use input data, they cannot do anything else like query database.\nAnd they cannot have any side effects, which means no matter when we run the function for a given input, the output should be the same.\nWhat’s more, mapreduce is a very low-level model for distributed execution, so engineers can implement higher-level query language base it, like SQL can be implemented as a pipeline of mapreduce.\n\nGraph Data ModelSuitable for Many-to-Many relationships.\n\nvertice &#x2F; node\nedge &#x2F; relation\nattribute\n\nGraph can store both homogeneous and heterogeneous data. For example, node can represents people, city, animal, activity, etc.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (3)OLTP, OLAP and Columnar Store","url":"/2023/08/21/DDIA-cookbook-3-OLTP-OLAP-and-Columnar-Store/","content":"OLTP &amp; OLAPOLTP &rarr; Online Transaction Processing.\nOLAP &rarr; Online Analytics Processing.\nA transaction needn’t have ACID. Transation processing just means allowing client to make low-latency reads and writes, which is opposed to batch processing.\nCompare\n\n\nProperty\nOLTP\nOLAP\n\n\n\nread\nsmall number of records per query, fetched by key\naggregate large amount of records\n\n\nwrite\nrandom access, low-latency\nbulk ELT or streaming\n\n\nprimary used by\nuesr, client\ninternal analyst for decision making\n\n\nwhat data represents\nlatest data state\nhistory of events that happened over time\n\n\ndata size\nsmall or medium\nlarge\n\n\nData Warehouse\nWarehouse will Extract-Transform-Load (ELT) data from multiple TP database and keep a read-only version data, so that when run AP in data warehouse, there is no inference with TP’s tasks.\nStars &#x2F; Snowflakes ModelStars model &rarr; center is the main table, and connect with several second level tables via foreign key\nSnowflakes model &rarr; similar to star model, but it may have more levels like third or fourth level to represent more detailed information\n\nColumn-orientedStorage Layout\nFor data warehouse, its main table may have 100+ even 1000+ columns, but for certain query, we just need roughly 2 ~ 3 columns for calculation. If we use row-oriented storage layout, it needs to fetch one row with 1000+ fields, then get only 2 ~ 3 data, then iter to next row. It has bad performance.\nIf we use column-oriented storage layout, it just need to fetch the required columns from disk and do aggregations.\nCompressionAnother advantage of column-oriented storage is: it can be compressed.\n\nfor column, it represents the same concept, so its domain may be small, for example, if column represents country, then it can only have 200 or 300 possible values\nfor column, it always has the same data type, like integer column, string column or boolean column, etc\n\nDue to these two properties, we can compress column via compression algorithm, e.g. Bitmask for bool, Run Length Encoding, etc.\nMemory bandwidthBecause column can be compressed, and for query we just fetch required columns without any useless fields, we can utilize memory bandwidth efficiently.\nVectorized processingSingle-Instruction-Multi-Data (a.k.a SIMD) technology can achieve vectorized processing, especially for bitwise operator, such as AND, OR, etc.\nv1 = [&quot;tom&quot;, &quot;bob&quot;, &quot;jack&quot;]bitmask = 0b010v1 SIMD bitmask = [None, &quot;bob&quot;, None]\n\nSort in column storage\nMost cases, we focus on aggregation of certain column, so the order is unnecessary\nSome cases, order can help us compress the column and do aggregation\nFor example, if we sort “age” column, then we can store the column as “(18, 100), (20, 50)” which means 100 rows have age&#x3D;18, 50 rows have age&#x3D;20.\n\n\nIt wouldn’t make sense to sort each column independently, because we will lose the information about which fields are in the same row\nDifferent queries benefit from different sort orders, so it can store the same data sorted in several ways\nFor example, in a cluster we have 3 machines, all of them can provide service. Then we can store data sorted by age in machine 1, sorted by name in machine 2, sorted by sex in machine 3.\n\n\n\nWriting to Column StorageAn update-in-place approach, like B-trees use, is not possible with compressed columns.\nWe can use LSM-Tree like method: All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn’t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk.\nAggregation: Data Cube &amp; Materialized ViewData warehouse queries often involve aggregations, such as COUNT, SUM, AVG, MIN, or MAX in SQL. If the same aggregates are used by many different queries, it can be wasteful to crunch through the raw data every time. So we can cache some of the counts or sums that queries use most often in disk.\nThe concept to persistent store something from memory into disk called materialize.\nThe materialized view is an actual copy of the query results, written to disk. When the underlying data changes, a materialized view needs to be updated, because it is a denormalized copy of the data.\nA common special case of a materialized view is known as a data cube, which is a grid of aggregates grouped by different dimensions.\n\n\nColumn familiy &ne; Column storageColumn family is a new concept for some databases like HBase and Cassandra.\nIts schema may looks like\n\n    \n        Id\n        Name\n        Work\n        Personal\n    \n    \n        Work.Phone\n        Work.Address\n        Personal.Phone\n        Personal.Address\n    \n    \n        1\n        Tom\n        xxx-xxx-xxxx\n        xxxx\n        yyy-yyy-yyyy\n        yyyy\n    \n    \n        2\n        Bob\n        xxx-xxx-xxxx\n        xxxx\n        yyy-yyy-yyyy\n        yyyy\n    \n\n\nThe column family means the Work.Phone and Work.Address are under Work family, the storage still uses row-oriented.\n","tags":["System Design","DDIA"]},{"title":"DDIA cookbook - (4)Encoding","url":"/2023/08/25/DDIA-cookbook-4-Encoding/","content":"CompatibilityMorden software development will use rolling upgrade or staged rollout, which means deploying the new version to a few nodes at a time, checking whether the new version is bug free or not, then gradually upgrade all nodes.\nAnd also we need to consider that our client may not install new version for some time.\nSo the new and old data formats, codes, policies will coexist in our system.\n\nBackward compatibility &rarr; newer data can read data that written by older code\nForward compatibility &rarr; older data can read data that written by newer code\n\n\nEncodingIn memory, we have various data structures like hash map, dictionary, vector, etc. But when we want to write data to a file or send it over network, we have to transform the complex data structure to simple sequence of bytes.\nThe process to transform data structure to bytes called encoding &#x2F; serialization; the process to translate bytes to data structure called decoding &#x2F; deserialization.\nThe problems are:\n\nhow can we encode &#x2F; decode data to save time and space\nhow can we make the format be compatible\n\nthrift and protobufThey are binary encoding libraries that are based on the same principle. Both Thrift and Protocol Buffers require a schema for any data that is encoded.\nstruct Person &#123;    1: required string userName,    2: optional i64 favoriteNumber,    3: optional list&lt;string&gt; interests &#125;\n\nmessage Person &#123;    required string user_name = 1;    optional int64 favorite_number = 2;    repeated string interests = 3;&#125;\n\n\nNote: this format called Interface Defination Language(IDL).\n\nField tags and schema evolutionEach field is identified by its tag number (the numbers 1, 2, 3 in the sample schemas) and annotated with a datatype (like int64, string, etc). You can change the name of a field in the schema, since the encoded data never refers to field names, but you cannot change a field’s tag, since that would make all existing encoded data invalid.\nYou can add new fields to the schema, provided that you give each field a new tag number and make it optional or has default value.\n\n\nforward: If old code (which doesn’t know about the new tag numbers you added) tries to read data written by new code, including a new field with a tag number it doesn’t recognize, it can simply ignore that field. The datatype annotation allows the parser to determine how many bytes it needs to skip\nbackward: The new code can always read old data, because the tag numbers still have the same meaning. The only detail is that if you add a new field, you cannot make it required. If you were to add a field and make it required, that check would fail if new code read data written by old code, because the old code will not have written the new field that you added.\n\n\nRemoving a field is just like adding a field, you can only remove a field that is optional and you can never use the same tag number again.\nThe merits of schemas\nThey can be much more compact than the various “binary JSON” variants, since they can omit field names from the encoded data.s\nThe schema is a valuable form of documentation, and because the schema is required for decoding, you can be sure that it is up to date.\nKeeping a database of schemas allows you to check forward and backward compatibility of schema changes, before anything is deployed.\nFor users of statically typed programming languages, the ability to generate code from the schema is useful, since it enables type checking at compile time.\n\n\nModes of DataflowDataflow through DatabasesFor database, it may be accessed by several processes, some requests are old, some are new, so compatibility is very important. Sometimes DBMS may alter the table schema to add or delete fields, which may cause problem:\n\ndifferent values written at different timesIn database, you may have some values that were written five milliseconds ago, and some values that were written five years ago. When you change your service code (e.g. change the encoding policy), the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it since then. This observation is sometimes summed up as data outlives code.\nRewriting (migrating) data into a new schema is certainly possible, but it’s an expensive thing to do on a large dataset.\nMost relational databases allow simple schema changes, such as adding a new column with a null default value, without rewriting existing data. When an old row is read, the database fills in nulls for any columns that are missing from the encoded data on disk.\narchive dataWhen you take a snapshot of your database from time to time, say for backup purposes or for loading into a data warehouse. In this case, the data dump will typically be encoded using the latest schema, even if the original encoding in the source database contained a mixture of schema versions from different eras.\n\nSince you’re copying the data anyway, you might as well encode the copy of the data consistently.\n\nDataflow through service callsMicroservices &rarr; make the application easier to change and maintain by making services independently deployable and evolvable.\n\nFor example, each service should be owned by one team, and that team should be able to release new versions of the service frequently, without having to coordinate with other teams. In other words, we should expect old and new versions of servers and clients to be running at the same time.\n\nRESTfulREST (Representational state transfer) is not a protocol, but rather a design philosophy that builds upon the principles of HTTP.\nTL;DR &rarr; use URL to locate resources, use HTTP verbs to describe actions, use HTTP status codes to indicate results\n\nwww.myblog.com/introduce is a webpage related to introduce myself; www.myblog.com/blog/python/python_intro is a blog to introduce python\n\n\nGET means fetch data from server, POST means submit form from client, DELETE means delete resources in server, etc (although POST can also be achieved by GET, but we need to clarify our actions)\n\nReference &rarr; What is RESTful API\nUniform interfaceIt indicates that the server transfers information in a standard format. The formatted resource is called a representation in REST. This format can be different from the internal representation of the resource on the server application. For example, the server can store data as text but send it in an HTML representation format.\nUniform interface imposes four architectural constraints:\n\nRequests should identify resources. They do so by using a uniform resource identifier (URI).\nClients have enough information in the resource representation to modify or delete the resource if they want to. The server meets this condition by sending metadata that describes the resource further.\nClients receive information about how to process the representation further. The server achieves this by sending self-descriptive messages that contain metadata about how the client can best use them.\nClients receive information about all other related resources they need to complete a task. The server achieves this by sending hyperlinks in the representation so that clients can dynamically discover more resources.\n\nStatelessnessIn REST architecture, statelessness refers to a communication method in which the server completes every client request independently of all previous requests. Clients can request resources in any order, and every request is stateless or isolated from other requests. This REST API design constraint implies that the server can completely understand and fulfill the request every time. \nLayered systemIn a layered system architecture, the client can connect to other authorized intermediaries between the client and server, and it will still receive responses from the server. Servers can also pass on requests to other servers. You can design your RESTful web service to run on several servers with multiple layers such as security, application, and business logic, working together to fulfill client requests. These layers remain invisible to the client.\nCacheabilityRESTful web services support caching, which is the process of storing some responses on the client or on an intermediary to improve server response time. For example, suppose that you visit a website that has common header and footer images on every page. Every time you visit a new website page, the server must resend the same images. To avoid this, the client caches or stores these images after the first response and then uses the images directly from the cache. RESTful web services control caching by using API responses that define themselves as cacheable or noncacheable.\nCode on demandIn REST architectural style, servers can temporarily extend or customize client functionality by transferring software programming code to the client. For example, when you fill a registration form on any website, your browser immediately highlights any mistakes you make, such as incorrect phone numbers. It can do this because of the code sent by the server.\nRPCProblems\nA local function call is predictable and either succeeds or fails, depending only on parameters that are under your control. RPC is unpredictable: the request or response may be lost due to a network problem, or the remote machine may be slow or unavailable, and such problems are entirely outside of your control.\nA local function call either returns a result, or throws an exception, or never returns. RPC has another possible outcome: it may return without a result, due to a timeout.\nIf you retry a failed network request, it could happen that the requests are actually getting through, and only the responses are getting lost. In that case, retrying will cause the action to be performed multiple times.\nEvery time you call a local function, it normally takes about the same time to execute. A network request is much slower than a function call, and its latency is also wildly variable.\nWhen you call a local function, you can efficiently pass it references (pointers) to objects in local memory. When you make a network request, all those parameters need to be encoded into a sequence of bytes that can be sent over the network.\nThe client and the service may be implemented in different programming languages.\n\nEvolutionFor services dataflow, it is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, we only need backward compatibility on requests, and forward compatibility on responses.\nService compatibility is made harder by the fact that RPC is often used for communication across organizational boundaries, so the provider of a service often has no control over its clients and cannot force them to upgrade.\nFor RESTful APIs, common approaches are to use a version number in the URL or in the HTTP Accept header.\nDataflow through asynchronous message passing\nService &rarr; one process sends a request over the network to another process and expects a response as quickly as possible\nDatabase &rarr; one process writes encoded data, and another process reads it again sometime in the future\n\nThe asynchronous message-passing systems, which are somewhere between RPC and databases.\nThey are similar to RPC in that a client’s request (usually called a message) is delivered to another process with low latency. They are similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a message broker (a.k.a. message queue), which stores the message temporarily.\n\none database &rarr; store message in queue + two RPC &rarr; sender with queue, queue with receiver\n\nUsing a message broker has several advantages compared to direct RPC:\n\nIt can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.\nIt can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.\nIt avoids the sender needing to know the IP address and port number of the recipient.\nIt allows one message to be sent to several recipients.\nIt logically decouples the sender from the recipient because the sender just publishes messages and doesn’t care who consumes them.\n\nHowever, a difference compared to RPC is that message-passing communication is usually one-way: a sender normally doesn’t expect to receive a reply to its messages. This communication pattern is asynchronous: the sender doesn’t wait for the message to be delivered, but simply sends it and then forgets about it.\nmessage brokerMessage brokers are used as follows: one process sends a message to a named queue or topic, and the broker ensures that the message is delivered to one or more consumers of or subscribers to that queue or topic.\n\nseveral consumers share one topic (mutual exclusive)\nevery consumer own its topic\n\nA topic provides only one-way dataflow. However, a consumer may itself publish messages to another topic like a chain, or to a reply queue that is consumed by the sender of the original message. So by combining several topics together, we can achieve complex topology.\n","tags":["System Design","DDIA"]},{"title":"Friend function inside template class","url":"/2023/08/09/Friend-function-inside-template-class/","content":"Assume we have a template class named Matrix, and we want to add a friend function called swap().\nMethod 1template&lt;typename T&gt;class Matrix &#123;    // ...    friend void swap(Matrix&lt;T&gt;&amp; a, Matrix&lt;T&gt;&amp; b) &#123;        // definition    &#125;&#125;;\n\nNotice, the swap() is not a template function! It’s just a normal function.\nWhy we need to define it inside the class?\n\nBecause when we create a Matrix&lt;int&gt;, it will also define a void swap(Matrix&lt;int&gt;&amp;, Matrix&lt;int&gt;&amp;) for system; for Matrix&lt;double&gt;, it’s the same. In other word, when we create certain type Matrix, the swap which supports that certain type will be automatically created for us. So that when we use swap(), it always make sure we can find the definition.\n\nCan we define it outside the class?\n\nYes, you can, but you need to make sure you write definitions for all possible types, like you need to implement a void swap(Matrix&lt;int&gt;&amp;, Matrix&lt;int&gt;), a void swap(Matrix&lt;double&gt;&amp;, Matrix&lt;double&gt;), etc for all possible typename T.\n\nMethod 2template&lt;typename T&gt;class Matrix;template&lt;typename T&gt;void swap(Matrix&lt;T&gt;&amp;, Matrix&lt;T&gt;&amp;);  // we need to know what is Matrix&lt;T&gt;, so declare Matrix beforetemplate&lt;typename T&gt;class Matrix &#123;    // ...    friend void swap&lt;&gt;(Matrix&amp; a, Matrix&amp; b);   // we need to know that is a template function, so declare before&#125;;template&lt;typename T&gt;void swap(Matrix&lt;T&gt;&amp; a, Matrix&lt;T&gt;&amp; b) &#123;    // definition&#125;\n\nNotice here the swap is a template function, so that we need to write it as swap&lt;&gt;, and we don’t need to explicitly denote Matrix type, cuz template funtion can deduce its type.\nCan we don’t write the declarition of template swap?\n\nNo. Inside class definition, we tell the class that we have a template friend function. If we don’t declare it before class definition, the class don’t know how many typenames it has, do it have default typename, etc.\n\nTricktemplate&lt;typename T&gt; void helper(T);template&lt;typename T&gt;class Obj &#123;    // ...    friend void helper&lt;char&gt;(char); // no matter what type Obj is, only helper with char type can get access to private members&#125;\n","tags":["c++"]},{"title":"DDIA cookbook - (5)Replication","url":"/2023/08/26/DDIA-cookbook-5-Replication/","content":"IntroWhy need multiple machines\nScalability &rarr; If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines.\nFault-tolerance&#x2F;high availability &rarr; If your application needs to continue working even if one machine goes down, you can use multiple machines to give you redundancy.\nLatency &rarr; If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them.\n\nScalingVertical ScalingVertical Scaling or Scaling up means upgrade to a more powerful machine.\nThere are two strategies:\n\nshared-memory &rarr; many CPUs use same memory and disk\nshared-disk &rarr; many CPUs and memories use same disk\n\nBut they have several problems:\n\nThe cost grows faster than linearly, add double hardware doesn’t mean double performance\nFor shared-memory, it’s limited to a single location\nFor shared-disk, the overhead of locking limit the scalability\n\nHorizontal scalingHorizontal Scaling or Scaling out treat every machine as node, each node uses its CPUs, RAM, and disks independently. Any coordination between nodes is done at the software level, using a conventional network.\nBecause every node is independent, so its strategy calls shared-nothing. It usually incurs additional complexity for applications and sometimes limits the expressiveness of the data models you can use.\nReplication Vs Partitioning\n\nReplication &rarr; Keeping a copy of the same data on several different nodes, potentially in different locations. Replication provides redundancy.\n\nPartitioning &rarr; Splitting a big database into smaller subsets called partitions so that different partitions can be assigned to different nodes (also known as sharding).\n\n\n\nLeader &amp; FollowerEach node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica; otherwise, the replicas would no longer contain the same data. The most common solution for this is called leader-follower&#x2F;master-slave replication.\n\n\nOne of the replicas is designated the leader (also known as master). When clients want to write to the database, they must send their requests to the leader.\nThe other replicas are known as followers (slaves or hot standbys). Whenever the leader get writes request, it also sends the data change to all of its followers as part of a replication log or change stream. Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.\nWhen a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader.\n\n\n\nSynchronous vs Asynchronous\n\nSync &rarr; leader waits until follower has confirmed that it received the write before reporting success to the user.\nAsync &rarr; leader sends the message, but doesn’t wait for a response from the follower.\n\nThe differences between sync and async are:\n\nsync sacrifices high availability to achieve strict consistency\nasync sacrifices strict consistency to achieve high availability\n\nHere are three configurations for different availability and consistency requirements:\n\nfully sync &rarr; leader finishes its write after all followers ack\nsemi sync &rarr; leader finishes its write after some of followers ack\nasync &rarr; leader finishes its write immediately, no need to wait follower ack\n\nSetting Up New FollowersFrom time to time, you need to set up new followers—perhaps to increase the number of replicas, or to replace failed nodes.\nWe cannot directly copy all current data files from other nodes because:\n\nclient will constantly write new data or update old data, if we only copy current data, it’s inconsistent\nif we block client’s write request, we can make all data consistent, but lose high availability\n\nThe correct apporach is:\n\nTake a consistent snapshot of the leader’s database at some point in time.\nCopy the snapshot to the new follower node.\nThe follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. The position of snapshot is sometimes called log sequence number or binlog coordinates.\nWhen the follower has processed the backlog of data changes since the snapshot, we say it has caught up. It can now continue to process data changes from the leader as they happen.\n\nHandling Node OutageFollower: catch-up recoveryEach follower keeps a log of the data changes it has received from the leader.\nThe follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected.\nLeader: failoverFailover\nDetermining that the leader has failed. The most common Failure detection algorithm uses timeout.\n\nChoosing a new leader. Leader election algorithm and Consensus algorithm: the leader is chosen by a majority of the remaining replicas and is usually the replica with the most up-to-date data changes from the old leader.\n\nReconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. The system needs to ensure that the old leader becomes a follower and recognizes the new leader.\n\n\nProblems\nIf asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If the former leader rejoins the cluster after a new leader has been chosen, the new leader may have received conflicting writes in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be discarded.\n\nDiscarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents.\n\nFor example current system needs to use Redis as cache, and the old leader wrote some primary key into Redis, then it crashed. The new leader also store its auto incremental primary key into Redis, but because its data is not up-to-date, so there may have some primary key already in Redis.\n\n\nIn certain fault scenarios, it could happen that two nodes both believe that they are the leader.\n\nHow to set a reasonable timeout value.\n\n\nImplementation of Replication LogsStatement-based replicationLeader records the SQL statements and send them to followers. The leader just acts like a client to there followers.\n\nFor example, the log may has recordSELECT * FROM table WHERE conditionsINSERT properties INTO table VALUES(values)\n\nThe disadvantages:\n\nNondeterministic &rarr; functions such as NOW() and RAND() are likely to generate a different value on each replica.\n\nExecution Order &rarr; if they use autoincrementing column, or if they depend on the existing data in the database, they must be executed in exactly the same order on each replica, or else they may have a different effect.\n\nSide effects &rarr; like triggers, stored procedures, user-defined functions may result in different side effects occurring on each replica, unless the side effects are absolutely deterministic.\n\n\nWrite-Ahead Log (WAL)WAL is used for recovery:\n\nIn the case of a log-structured storage engine, every modification is first written to a write-ahead log so that the memtable can be recovered even the crash happens.\n\nIn the case of a B-tree, which overwrites individual disk blocks, every modification is first written to a write-ahead log so that the index can be restored to a consistent state after a crash.\n\n\nIt’s append-only sequence of bytes. The disadvantage is:\n\nWAL contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine.\n\nIf the storage engine changes or is incompatible, WAL may cannot allocate the data to certain position.\nLogical Log (binlog in MySQL)A log should be decoupled from the storage engine.\nA logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row:\n\nFor an inserted row, the log contains the new values of all columns.\nFor a deleted row, the log contains enough information to uniquely identify the row that was deleted. (tombstone)\nFor an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns.\nFor transaction, the log will append a flag to inform the commit of the transaction.\n\nProblems with Replication LagIf we have multiple followers, we can get:\n\navailablity &rarr; tolerate some faults in other machine\nscalability &rarr; read can be distributed so we can deal with more requests\nlow-latency &rarr; request can choose a fast path\n\nBut the question is: how to make all replics look the same.\nIf we choose sync replication, all problems can be solved except that if some replics crash, the service is blocked to wait for replics recovery, which may cause user complain and is unacceptable.\nSo the only way is async replication. But it may has a problem: some replics may fall behind other replics.\nThis leads to apparent inconsistencies in the database: if you run the same query on the leader and a follower at the same time, you may get different results, because not all writes have been reflected in the follower.\nGood news is this inconsistency is just a temporary state—if you stop writing to the database and wait a while, the followers will eventually catch up and become consistent with the leader. For that reason, this effect is known as eventual consistency.\nReading Your Own Writes\nIn this situation, we need read-after-write consistency. This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time.\nMethods\nWhen reading something that the user may have modified, read it from the leader; otherwise, read it from a follower.\nTrack the time of the last update and, if new requests within certain time threshold, make all reads from the leader.\nThe client can remember the timestamp of its most recent write—then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp.\n\nProblems\nTimestamp is very hard to sync for one logic user has multiple physical device.\nReplicas are distributed across different datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter.s\n\nMonotonic Reads\nMonotonic reads is a guarantee that is a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency. When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.\n\nFor example, the old data is 1 &rarr; 2, and right now the newest data is 3, when we send the first read, it may return “2”, then I send several reads, it makes sure that the responses look like “2, 2, 2, …, 3, 3, 3”. “1” will never appear and if we see “3”, “2” will never appear.\n\nMethods\neach user always makes their reads from the same replica (different users can read from different replicas).\ntimestamp\n\nDifference with “Reading your own writes”“Reading your own writes” guarantees the read order after write, “Monotonic reads” guarantees multiple reads order.\nConsistent Prefix Reads (Causal)\nThe reason for inconsistent prefix is partition. The order inside one partition is easy to maintain, but inter partition is hard.\nMethods\nno partition.\nroute all causal requests to same partition, but how to detect several requests are causal is hard.\n\nSolutions for Replication LagTransaction!!! We will cover it later.\n\nMulti-leaders","tags":["System Design","DDIA"]},{"title":"Unittest in python - Patch","url":"/2023/07/27/Unittest-in-python-Patch/","content":"After introduced mock in previous section, let’s talk about another powerful module – patch.\nIntropatch is a decorator &#x2F; context manager, and it can help you use some new stuff (default is Mock()) to replace target.\ndirectly use# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patchdef main():    mock_func = patch(&quot;demo.func&quot;)    mock_func.return_value = 10    mock_func.start()    demo.func()     # 10    mock_func.end()    demo.func()     # 1\n\ncontext manager# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patchdef main():    with patch(&quot;demo.func&quot;) as mock_func:        mock_func.return_value = 10        demo.func() # 10    demo.func()     # 1\n\ndecorator# demo.pydef func():    return 1\n\nimport demofrom unittest.mock import patch@patch(&quot;demo.func&quot;)def test_main(mock_func):    mock_func.return_value = 10    demo.func() # 10\n\nnotice 0Please notice, if we have multiple @patch for one function, the order is important: the inner decorator decorates front parameter.\n@patch(&quot;demo.func2&quot;)@patch(&quot;demo.func1&quot;)def test(mock_func1, mock_func2):    ...\n\nnotice 1Decorator can also be used in class. Remember patch is used for test, so\n\nonly function name starts with test_ will be treat as test function\nonly class derives from unittest.TestCase will be treat as test class\n\n@patch(&quot;demo.func&quot;)class MyTest(unittest.TestCase):    def test_1(self, mock_func):        ...    def test_2(self, mock_func):        ...        # patch is not working    def mytest_func(self):        ...\n\n\npatchunittest.mock.patch(target, new=DEFAULT, spec=None, create=False, spec_set=None, autospec=None, new_callable=None, **kwargs) \n\ntarget: target object’s path, remember it must be a string looks like package.module.className. If the object is defined in the same file, please use __main__.className.\nnew: default is MagicMock(), it can be a value or a actual object.\nnew_callable: it is a callable to create object.\nspec &amp; spec_set: please refer Mock parts\n\ndef new_func():    return 10def main():    mock_func = patch(&quot;demo.func&quot;, new = new_func)    mock_func.start()    demo.func()     # new_func() -&gt; 10    mock_func.end()\n\npatch target pathIf we want to patch some function, the path is not where we define the function, is where we use it.\n# package2.m2.pyfrom package1.m1 import func1def func2():    func1()\n\n# test.py@patch(&quot;package2.m2.func1&quot;)\n\nnew vs return_valuedef main():    mock_func = patch(&quot;demo.func&quot;, return_value = 10)    mock_func.start()    demo.func() # 10    mock_func.end()def main():    mock_func = patch(&quot;demo.func&quot;, new = 10)    mock_func.start()    demo.func   # 10    mock_func.end()\n\nnew vs new_callablenew is an instance, new_callable is a callable to create instance.\nfoo = 10with patch(&quot;__main__.foo&quot;, new = 100):    foowith patch(&quot;__main__.foo&quot;, new_callable = lambda: 100):    foo\n\ndef return_100():    return 100def main():    mock_func = patch(&quot;demo.func&quot;, new = return_100)    mock_func.start()    demo.func() # 100    mock_func.end()# don&#x27;t recommend to assign a value to newdef main():    mock_func = patch(&quot;demo.func&quot;, new_callable = return_100)    mock_func.start()    demo.func   # 100    mock_func.end()\n\nCannot use ‘new’ and ‘new_callable’ together!\npatch(&quot;demo.func&quot;, new = xxx, new_callable = xxx)   # error\n\nhow to patch a whole class# util.pyclass Object:    def __init__(self, x, y):        self.x = x        self.y = y        def show(self):        print(f&quot;x = &#123;x&#125;, y = &#123;y&#125;&quot;)        return 0\n\n# keep this the same as __init__def constructor(self, x, y):    self = Mock(spec = Object)    self.x = 2 * x    self.y = 2 * y    self.show.return_value = 100    return self@patch(&quot;util.Object&quot;, new = constructor)def test_patch_with_new():    o = Object(10, 10)    res = o.show()      # x = 20, y = 20    print(res)          # 100    o.x = 1000    print(o.x)          # 1000\n\n\npatch.objectUsed to mock methods in one class.\nimport Object# only mock func inside Object class@patch.object(Object, &quot;func&quot;)def test_patch_object(mock_func):    mock_func.return_value = 100    o = Object()    o.func()    # 100\n\n\n\npatch.dictm = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;with patch.dict(m, &#123;&quot;a&quot;: 10, &quot;b&quot;: 20&#125;, clear=True):    m[&quot;a&quot;]  # 10\n","tags":["python","test","pytest"]},{"title":"Unittest in python - Mock","url":"/2023/07/27/Unittest-in-python-Mock/","content":"IntroMock is a very useful package for unittest in python. It can replace some classes or functions and change their behaviors, it can also use some built-in methods to help you assert whether pytest calls certain part of your code.\n\nMock() &amp; MagicMock()\nFor simplicity, let’s use Mock for example. In most cases, Mock and MagicMock are the same :)\n\nMock is a class that create all attributes and methods as you access them and store details of how they have been used.\nWhat’s more, you can set anything to a Mock, it will treat them as new Mock (sub Mock).\n# set an undefined method to a Mockm = Mock()              # &lt;Mock name=&quot;mock&quot;&gt;m.undefined_function()  # &lt;Mock name=&quot;mock.undefined_function()&quot;&gt;# use mock as a argumentclass Object:    def func(self, args):        args.do_something()o = Object()m = Mock()o.func(m)m.do_something()    # &lt;Mock name=&quot;mock.do_something()&quot;&gt;\n\nreturn_valueBy setting some methods or functions as Mock, then setting return_value can change original logic: I don’t care about what you write in the function, just return what I want!\nclass Object:    def __init__(self, x):        self.x = x        def func(self):        return self.xobj = Object(1)# method 1obj.func = Mock(return_value = 1024)# # method 2# obj.func = Mock()# obj.func.return_value = 1024obj.func()  # return 1024 rather than 1\n\nThis is always useful in unittest, like:\n\nI don’t want to send a real request via network, just let the requester &#x2F; dispatcher return what I want;\nI don’t want to access a real DB, just tell me what data you have;\n\nclass MySvc:    def __init__(self, db):        self.db = db        ...    def myRequest(self, req):        ...        results = self.db.fetch(req)        return resultsdef test_db(req):    db = MyDB()    db.fetch = Mock(return_value = [data1, data2, data3, ...])    svc = MySvc(db)    results = svc.myRequest(req)    # [data1, data2, data3, ...]\n\nSometimes, we will meet some call chains, such as mock.connection.cursor().execute(...).\n# mock.call1().call2().call3()m = Mock()# get Mock for all calls except the last onec1 = m.call1.return_valuec2 = c1.call2.return_value# set Mock for last one callc2.call3.return_value = &quot;foo&quot;m.call1().call2().call3()   # &quot;foo&quot;\n\n\nBasically, we can change the code as following\nm = Mock()c1 = Mock()m.call1.return_value = c1c2 = Mock()c1.call2.return_value = c2c2.call3.return_value = &quot;foo&quot;\n\nside_effectside_effect &#x3D; Exception&gt;&gt; m = Mock()&gt;&gt; m.exception_side_effect = Mock(side_effect = ValueError)&gt;&gt; m.exception_side_effect()ValueError\n\nside_effect &#x3D; iterableIf we set iterable to side_effect, every time we call it, it will yield one element.\n&gt;&gt; m = Mock()&gt;&gt; m.iter = Mock(side_effect = [1, 2, 3])&gt;&gt; m.iter()1&gt;&gt; m.iter()2&gt;&gt; m.iter()3\n\nside_effect &#x3D; callabledef log(*args, **kwargs):    print(f&quot;args: &#123;args&#125;, kwargs: &#123;kwargs&#125;&quot;)&gt;&gt; m = Mock()&gt;&gt; m.func = Mock(side_effect = log)&gt;&gt; m.func()args: (), kwargs: &#123;&#125;&gt;&gt; m.func(1, two = 2)args: (1,), kwargs: &#123;&quot;two&quot;: 2&#125;\n\nWhen we set both return_value and side_effect, the Mock will only use side_effect!!\nspec &amp; spec_setspec can be either a list of string or an existing class &#x2F; instance. After we set spec, the mock can only have corresponding attributes and methods (just like we use dir to see what attributes and methods does one class support).\nclass Object:    def __init__(self):        self.one = 1        self.two = 2    def func(self):        pass# when we use existing class as spec, the mock hasn&#x27;t initedm = Mock(spec = Object)m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # error, cuz we don&#x27;t init the Objectm.__init__()m.one       # 1m.three = 3 # ok# when we use existing instance as spec, the mock has initedo = Object()m = Mock(spec = o)m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # 1# when we use list of string as specm = Mock(spec = [&quot;one&quot;, &quot;func&quot;])m.func()    # &lt;Mock name=&quot;mock.func()&quot;&gt;m.one       # &lt;Mock name=&quot;mock.one&quot;&gt;m.one()     # &lt;Mock name=&quot;mock.one()&quot;&gt;\n\nThe difference between spec and spec_set is, spec can add new stuff while spec_set can only read.\nm = Mock(spec = [&quot;one&quot;])m.onem.two = 2   # okmm = Mock(spec_set = [&quot;one&quot;])m.onem.two = 2   # error\n\nassertion &amp; call argsMock supports lots of assertions, such as assert_called, assert_called_once, assert_called_with, etc.\nm = Mock()m(1, 2)m.assert_called()           # Truem.assert_called_with(1, 2)  # True\n\nMock can also remember what args you used via call_args or call_args_list.\nm = Mock()m(1, 2)m.call_args         # call(1, 2)m.call_args_list    # [call(1, 2)]m(3, 4)m.call_args         # call(3, 4)m.call_args_list    # [call(1, 2), call(3, 4)]\n\nWhat’s the difference between these two?So you can simply think MagicMock &#x3D; Mock with pre-defined magic methods.\n&gt;&gt; len(Mock())Error, Mock doesn&#x27;t have __len__ method&gt;&gt; len(MagicMock())0\n\nSo if you want to test or use magic methods in your test, use MagicMock.\nIf you want to modify the magic methods or just for simplicity purpose, plz use Mock.\n","tags":["python","test","pytest"]},{"title":"Why and why not inline","url":"/2023/07/20/Why-and-why-not-inline/","content":"Why inlineIn C++, we can add inline keyword in front of function defination (inline only works on defination, you don’t need to use inline in statement).\nvoid this_is_inline_func();inline void this_is_inline_func() &#123;    std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;&#125;\n\nno inlineWhen we call a function, the process is\n\nfind function defination and its address\nload from memory to stack\nexecute\npop from stack\n\nvoid this_is_not_inline() &#123;    ...&#125;this_is_not_inline();   // 1this_is_not_inline();   // 2this_is_not_inline();   // 3\n\n0x0000 -&gt; defination of this_is_not_inline...0x0100 -&gt; call 1    // find address is 0x0000 -&gt; push 0x0000 into stack -&gt; execute -&gt; pop0x0104 -&gt; call 2    // same0x0108 -&gt; call 3    // same\n\ninlineAll of these steps will cost extra time. To avoid this cost, we can use inline. After we defining a function as inline, we may have multiple calls in code. When we compile the code, the compiler will change function calls to function defination, so that when execute the code, instead of wasting time finding defination and loading from memory to stack, it just executes the function logic.\nvoid this_is_inline() &#123;    std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;&#125;this_is_inline();   // 1this_is_inline();   // 2this_is_inline();   // 3\n\n0x0000 -&gt; defination of this_is_inline...0x0100 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;  // change call to defination0x0200 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;0x0300 -&gt; std::cout &lt;&lt; &quot;this is inline func&quot; &lt;&lt; std::endl;\n\n\nWhy not inlineSounds great? But the problem is, we need to copy the defination after every calls. If the defination is very complicated, we will cost losts of memory. What’s more, if the function is complicated, the execution time &gt;&gt; load &amp; pop stack time, we can ignore the inline improvement.\n\nSuggestionOnly use inline for very simple function. Actually inline is just a recommendation rather than requirement. If the function contains loop, recursion, static, etc complex logic, the function will be treated as not-inline even if you add inline keyword.\nWhat’s more, in class, the methods defination will be treated as inline. So only define simple methods in class, for complex methods, leave statements inside class, then define them outside class.\nclass Object &#123;public:    Object() =default;    Object(const Object&amp; rhs) &#123; num = rhs.num; &#125;    int get_num() &#123; return num; &#125;   // define simple method inside class (auto inline)    void inline_function();    void complex_function();private:    int num;&#125;;// we can still define inline outside classinline void Object::inline_function() &#123;    ...&#125;// for complex method, plz define outside classvoid Object::complex_function() &#123;    ...&#125;\n\n\n#define vs inline#define replaces the pure text in pre-complie stage, inline replaces function call with function defination in complie stage.\n","tags":["c++"]},{"title":"#include <> Vs #include \"\" ","url":"/2023/07/16/cpp_include_difference/","content":"#include &lt;header&gt;This will let preprocessor find header files in pre-designated directories.\n\nThese directories are normally system-related, such as “&#x2F;usr&#x2F;include”, “&#x2F;x86&#x2F;include”, etc.\n\nLet’s use Linux as example. If we ls /usr, you will see bin, include and lib, and if we ls /usr/include, output will contain lots of system defined header files, such as /usr/include/cpp/vector and /usr/include/cpp/iostream. This is how we include these commonly used packages.\n// find header file named &quot;iostream&quot; from &quot;/usr/include&quot;#include &lt;iostream&gt;\n\n\n#include &quot;header&quot;This will search programmer-defined header files and typically includes same directory as the file containing the directive.\nmy_project| - include|   | - my_header.h| - my_project.cpp\n// my_project.cpp#include &quot;my_header.h&quot;\n\nLet’s assume the project tree looks like that, and when we #include &quot;&quot;, it will search header files under /my_project directory.\nIf the processor cannot find header file in current project directory, it will search header files in system path (the same as #include &lt;&gt;).\n\n#include &quot;header.h&quot; – cannot find locally –&gt; #include &lt;header.h&gt;\n\n\nAdd include pathAs we discussed before, #include &lt;&gt; will search pre-designated directories. What if we want the processor to search other directories?\nWe can add include paths via \n\n-I\nsystem environment variable\nCMake\n\nAfter adding include paths, the searching order is\n\n#include &lt;&gt;\ndir list\nsystem dir\n\n\n#include &quot;&quot;\nlocal\ndir list\nsystem dir\n\n\n\n-IWe can use -I flag in command line to add search paths.\ng++ -c test.cpp -o testg++ -c test.cpp -o test -I /path/to/certain/include/\n\nSystem environment variableThe system environment has a property called CPLUS_INCLUDE_PATH, we can set this to add search paths.\nexport CPLUS_INCLUDE_PATH=&quot;path/to/certain/include&quot;\n\nCMakeFor CMake, we can use include_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...]) .\n","tags":["c++"]},{"title":"How to setup blog?","url":"/2023/07/14/how_to_setup_blog/","content":"GithubJust create a new repo, and the repo’s name follows rule “username.github.io”.\n\nNode.js &amp; HexoInstallJust donwload Node.js from offical website (maybe you also need npm :)). For Mac user, the simplest way is run command:\nbrew install node\n\nFor Hexo, we just need to run\nnpm install hexo-cli -g\n\nAfter installation, use commands to validate:\nnode -vnpm -vhexo -v\n\nInitIf everything is OK, run following commands:\nmkdir blog &amp;&amp; cd bloghexo init   # init the folder as blog reponpm install # install necessary node_modules\n\nThen you will find a file named blog/_config.yml, add these info in last rows:\n# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy:  type: git  repository: &lt;your github repo&gt;  branch: master\n\nDeployYou can add a new plug-in npm i hexo-deployer-git so that we can post blogs to git via hexo.\nhexo new post &quot;this is post name&quot;hexo g  # generatehexo s  # can view changes in localhosthexo d  # deploy to github\n\n\n:money:\nActually this step is optional, cause u can use “username.github.io” to browse your blog. However, if you want a fancy URI (and you are rich), you can follow the step.\n\nYou should buy a web URI, I got one from GoDaddy.\nAfter you got one, let’s say it’s “myfancyblog.com”, you should set the DNS so that when other people enter “myfancyblog.com” in browser, it can re-direct to “username.github.io”.\nOther people                  Your github repo&quot;myfancyblog.com&quot;   - DNS -&gt;  &quot;username.github.io&quot;\n\nIn DNS Management page, you should set two more DNS Records.\n\n\n\nType\nName\nData\n\n\n\nA\n@\nIP Address For username.github.io\n\n\nCNAME\nwww\nusername.github.io\n\n\nThe first one re-directs “myfancyblog.com” to “username.github.io”‘s IP, the second re-directs “www.myfancyblog.com“ to “username.github.io”.\nThen, you should go back to your blog repo, in blog/source create a new file CNAME, then put “myfancyblog.com” in this file. hexo g &amp;&amp; hexo d. You will see github.com will have a new CNAME file, and you can check “Setting -&gt; Pages”, it should have “myfancyblog.com” in “Custom domain” field.\n\nTipsIf you are curious about “why I changed some settings but the web page seems no change”, :(, well, remember to clear the browser cache.\n","tags":["blog","tools"]},{"title":"tmux cookbook","url":"/2023/07/17/tmux-cookbook/","content":"TmuxWhen we use command line tools (CLI), we will open a terminal, and input some commands. This process called “session”. But session is temporary, which means when we close the terminal, the session ends.\nHow can we keep the session even we close the page?\nUsing Tmux :)\nTmux will create a new terminal (you can think this is a sub-terminal), and we can run commands inside. When we close the terminal, it will not end the session, just return back to the main terminal, we can connect the sub-terminal again via tmux.\nWhen use tmux?This is a difficult question. For me, I will use tmux in 2 situations:\n\nI want to show off. You know what I mean, multiple panes in one screen, especially use one pane to run top command, it’s just like hack style.\nThe task will take a long running time. For instance, the machine learning stuff, you can run it in one session, then go back to working. After one million year, re-connect the session and see error information like “No Module Named xxx” (just don’t ask me why I know that)\n\ncookbook\n\n\nCommand\nFunctionality\n\n\n\ntmux new -s &lt;name&gt;\ncreate a named session\n\n\ntmux detach\nkeep the session and go back to main terminal\n\n\ntmux ls\nlist sessions\n\n\ntmux attach -t &lt;name&gt;\nre-connect to session\n\n\ntmux kill-session -t &lt;name&gt;\nend session\n\n\ntmux switch -t &lt;name&gt;\nswitch to session\n\n\ntmux split-window\nsplit up &amp; down\n\n\ntmux split-window -h\nsplit left &amp; right\n\n\ntmux select-pane -U&#x2F;D&#x2F;L&#x2F;R\nselect up&#x2F;down&#x2F;left&#x2F;right pane\n\n\ntmux swap-pane -U&#x2F;D\nchange pane’s size\n\n\nhot key\n\n\nhot key\nfunctionality\n\n\n\nctrl+d\nexit and kill session\n\n\nctrl+b d\ntmux detach\n\n\nctrl+b ?\nhelp\n\n\nctrl+b s\ncheck all sessions\n\n\nctrl+b $\nrename session\n\n\nctrl+b %\nsplit left &amp; right\n\n\nctrl+b “\nsplit up &amp; down\n\n\nctrl+b x\nkill session\n\n\nctrl+b &lt;arrow key&gt;\nselect pane\n\n\nctrl+b ctrl+&lt;arrow key&gt;\nresize pane\n\n\nctrl+b !\nmake panes into different sessions\n\n\nctrl+b :set &lt;command&gt;\nset properties, eg: :set mouse on to enable mouse control\n\n\n","tags":["tools","cookbook"]}]